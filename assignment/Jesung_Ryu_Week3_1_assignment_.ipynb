{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugGR7pnI4WSe"
      },
      "source": [
        "# Week3_1 Assignment\n",
        "\n",
        "## [BASIC](#Basic) \n",
        "- 토크나이징이 완료된 위키 백과 코퍼스를 다운받고 **단어 사전을 구축하는 함수를 구현**할 수 있다.\n",
        "- `Skip-Gram` 방식의 학습 데이터 셋을 생성하는 **Dataset과 Dataloader 클래스를 구현**할 수 있다.\n",
        "- **Negative Sampling** 함수를 구현할 수 있다. \n",
        "\n",
        "\n",
        "## [CHALLENGE](#Challenge)\n",
        "- Skip-Gram을 학습 과정 튜토리얼을 따라하며, **Skip-Gram을 학습하는 클래스를 구현**할 수 있다. \n",
        "\n",
        "\n",
        "## [ADVANCED](#Advanced)\n",
        "- Skip-Gram 방식으로 word embedding을 학습하는 **Word2Vec 클래스를 구현**하고 실제로 학습할 수 있다.\n",
        "- 학습이 완료된 word embedding을 불러와 **Gensim 패키지를 사용해 유사한 단어**를 뽑을 수 있다. \n",
        "\n",
        "### Reference\n",
        "- [Skip-Gram negative sampling 한국어 튜토리얼](https://wikidocs.net/69141)\n",
        "    - (참고) 위 튜토리얼에서는 target word와 context word 페어의 레이블은 1로, target word와 negative sample word 페어의 레이블은 0이 되도록 학습 데이터를 구현해 binary classification을 구현한다. 하지만 우리는 word2vec 논문 방식을 그대로 따르기 위해 label을 생성하지 않고 대신 loss 함수를 변행해서 binary classification을 학습할 것이다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:29:36.641276Z",
          "start_time": "2022-02-19T14:29:36.638642Z"
        },
        "id": "HlEy3xfY4WSh"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from typing import List, Dict\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:50:41.644583Z",
          "start_time": "2022-02-19T12:50:41.642937Z"
        },
        "id": "cBrr7-gt4jnf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1be2dcf4-735f-4508-be04-f323417c098b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:26:59.276355Z",
          "start_time": "2022-02-19T14:26:58.411434Z"
        },
        "id": "6mC9lhsJ4WSh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import SGD\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:30:05.586472Z",
          "start_time": "2022-02-19T14:30:05.583611Z"
        },
        "id": "17g7UZ5g4WSi"
      },
      "outputs": [],
      "source": [
        "# seed\n",
        "seed = 7777\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:30:06.721039Z",
          "start_time": "2022-02-19T14:30:06.717559Z"
        },
        "id": "v3UlC7Jn4WSi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f035d88-a1d5-4ecc-f2fc-1f8de0d5e0da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla P100-PCIE-16GB\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# device type\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8sfv5KY4WSk"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHs8_LU04WSj"
      },
      "source": [
        "### 토크나이징이 완료된 위키 백과 코퍼스 다운로드 및 불용어 사전 크롤링\n",
        "- 나의 구글 드라이브에 데이터를 다운받아 영구적으로 사용할 수 있도록 하자. \n",
        "    - [데이터 다운로드 출처](https://ratsgo.github.io/embedding/downloaddata.html)\n",
        "- 다운받은 데이터는 토크나이징이 완료된 상태이지만 불용어를 포함하고 있다. 따라서 향후 불용어를 제거하기 위해 불용어 사전을 크롤링하자. \n",
        "    - [불용어 사전 출처](https://www.ranks.nl/stopwords/korean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYiz1fdNsAqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df5cf16a-0366-4952-97ae-cfae7a2dbb9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2WZ0P4wsAqp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c7fecb5-792d-4611-cd67-a9ed093c04d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/[Wanted]week3-1\n"
          ]
        }
      ],
      "source": [
        "cd  /content/drive/MyDrive/[Wanted]week3-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:11.886643Z",
          "start_time": "2022-02-19T14:27:11.884858Z"
        },
        "id": "4QPBJ6UZ4WSj"
      },
      "outputs": [],
      "source": [
        "# 데이터 다운로드\n",
        "# !pip install gdown\n",
        "# !gdown https://drive.google.com/u/0/uc?id=1Ybp_DmzNEpsBrUKZ1-NoPDzCMO39f-fx\n",
        "\n",
        "\n",
        "# 권한 문제로 링크로부터 별도로 다운받은 후 옮기기\n",
        "# !unzip tokenized.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:15.633947Z",
          "start_time": "2022-02-19T14:27:13.829982Z"
        },
        "id": "cTHCHmO24WSj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68571538-4b01-4c81-ccc6-696675f9a123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Korean stop words: 677\n"
          ]
        }
      ],
      "source": [
        "# 한국어 불용어 리스트 크롤링\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.ranks.nl/stopwords/korean\"\n",
        "response = requests.get(url, verify = False)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text,'html.parser')\n",
        "    content = soup.select_one('#article178ebefbfb1b165454ec9f168f545239 > div.panel-body > table > tbody > tr')\n",
        "    stop_words=[]\n",
        "    for x in content.strings:\n",
        "        x=x.strip()\n",
        "        if x:\n",
        "            stop_words.append(x)\n",
        "    print(f\"# Korean stop words: {len(stop_words)}\")\n",
        "else:\n",
        "    print(response.status_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:27:15.642775Z",
          "start_time": "2022-02-19T14:27:15.635333Z"
        },
        "id": "3d0IqhDF4WSk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2286f8a4-6b45-4c07-9d4b-c19867de5ad6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'가',\n",
              " '가까스로',\n",
              " '가령',\n",
              " '각',\n",
              " '각각',\n",
              " '각자',\n",
              " '각종',\n",
              " '갖고말하자면',\n",
              " '같다',\n",
              " '같이',\n",
              " '개의치않고',\n",
              " '거니와',\n",
              " '거바',\n",
              " '거의',\n",
              " '것',\n",
              " '것과 같이',\n",
              " '것들',\n",
              " '게다가',\n",
              " '게우다',\n",
              " '겨우',\n",
              " '견지에서',\n",
              " '결과에 이르다',\n",
              " '결국',\n",
              " '결론을 낼 수 있다',\n",
              " '겸사겸사',\n",
              " '고려하면',\n",
              " '고로',\n",
              " '곧',\n",
              " '공동으로',\n",
              " '과',\n",
              " '과연',\n",
              " '관계가 있다',\n",
              " '관계없이',\n",
              " '관련이 있다',\n",
              " '관하여',\n",
              " '관한',\n",
              " '관해서는',\n",
              " '구',\n",
              " '구체적으로',\n",
              " '구토하다',\n",
              " '그',\n",
              " '그들',\n",
              " '그때',\n",
              " '그래',\n",
              " '그래도',\n",
              " '그래서',\n",
              " '그러나',\n",
              " '그러니',\n",
              " '그러니까',\n",
              " '그러면',\n",
              " '그러므로',\n",
              " '그러한즉',\n",
              " '그런 까닭에',\n",
              " '그런데',\n",
              " '그런즉',\n",
              " '그럼',\n",
              " '그럼에도 불구하고',\n",
              " '그렇게 함으로써',\n",
              " '그렇지',\n",
              " '그렇지 않다면',\n",
              " '그렇지 않으면',\n",
              " '그렇지만',\n",
              " '그렇지않으면',\n",
              " '그리고',\n",
              " '그리하여',\n",
              " '그만이다',\n",
              " '그에 따르는',\n",
              " '그위에',\n",
              " '그저',\n",
              " '그중에서',\n",
              " '그치지 않다',\n",
              " '근거로',\n",
              " '근거하여',\n",
              " '기대여',\n",
              " '기점으로',\n",
              " '기준으로',\n",
              " '기타',\n",
              " '까닭으로',\n",
              " '까악',\n",
              " '까지',\n",
              " '까지 미치다',\n",
              " '까지도',\n",
              " '꽈당',\n",
              " '끙끙',\n",
              " '끼익',\n",
              " '나',\n",
              " '나머지는',\n",
              " '남들',\n",
              " '남짓',\n",
              " '너',\n",
              " '너희',\n",
              " '너희들',\n",
              " '네',\n",
              " '넷',\n",
              " '년',\n",
              " '논하지 않다',\n",
              " '놀라다',\n",
              " '누가 알겠는가',\n",
              " '누구',\n",
              " '다른',\n",
              " '다른 방면으로',\n",
              " '다만',\n",
              " '다섯',\n",
              " '다소',\n",
              " '다수',\n",
              " '다시 말하자면',\n",
              " '다시말하면',\n",
              " '다음',\n",
              " '다음에',\n",
              " '다음으로',\n",
              " '단지',\n",
              " '답다',\n",
              " '당신',\n",
              " '당장',\n",
              " '대로 하다',\n",
              " '대하면',\n",
              " '대하여',\n",
              " '대해 말하자면',\n",
              " '대해서',\n",
              " '댕그',\n",
              " '더구나',\n",
              " '더군다나',\n",
              " '더라도',\n",
              " '더불어',\n",
              " '더욱더',\n",
              " '더욱이는',\n",
              " '도달하다',\n",
              " '도착하다',\n",
              " '동시에',\n",
              " '동안',\n",
              " '된바에야',\n",
              " '된이상',\n",
              " '두번째로',\n",
              " '둘',\n",
              " '둥둥',\n",
              " '뒤따라',\n",
              " '뒤이어',\n",
              " '든간에',\n",
              " '들',\n",
              " '등',\n",
              " '등등',\n",
              " '딩동',\n",
              " '따라',\n",
              " '따라서',\n",
              " '따위',\n",
              " '따지지 않다',\n",
              " '딱',\n",
              " '때',\n",
              " '때가 되어',\n",
              " '때문에',\n",
              " '또',\n",
              " '또한',\n",
              " '뚝뚝',\n",
              " '라 해도',\n",
              " '령',\n",
              " '로',\n",
              " '로 인하여',\n",
              " '로부터',\n",
              " '로써',\n",
              " '륙',\n",
              " '를',\n",
              " '마음대로',\n",
              " '마저',\n",
              " '마저도',\n",
              " '마치',\n",
              " '막론하고',\n",
              " '만 못하다',\n",
              " '만약',\n",
              " '만약에',\n",
              " '만은 아니다',\n",
              " '만이 아니다',\n",
              " '만일',\n",
              " '만큼',\n",
              " '말하자면',\n",
              " '말할것도 없고',\n",
              " '매',\n",
              " '매번',\n",
              " '메쓰겁다',\n",
              " '몇',\n",
              " '모',\n",
              " '모두',\n",
              " '무렵',\n",
              " '무릎쓰고',\n",
              " '무슨',\n",
              " '무엇',\n",
              " '무엇때문에',\n",
              " '물론',\n",
              " '및',\n",
              " '바꾸어말하면',\n",
              " '바꾸어말하자면',\n",
              " '바꾸어서 말하면',\n",
              " '바꾸어서 한다면',\n",
              " '바꿔 말하면',\n",
              " '바로',\n",
              " '바와같이',\n",
              " '밖에 안된다',\n",
              " '반대로',\n",
              " '반대로 말하자면',\n",
              " '반드시',\n",
              " '버금',\n",
              " '보는데서',\n",
              " '보다더',\n",
              " '보드득',\n",
              " '본대로',\n",
              " '봐',\n",
              " '봐라',\n",
              " '부류의 사람들',\n",
              " '부터',\n",
              " '불구하고',\n",
              " '불문하고',\n",
              " '붕붕',\n",
              " '비걱거리다',\n",
              " '비교적',\n",
              " '비길수 없다',\n",
              " '비로소',\n",
              " '비록',\n",
              " '비슷하다',\n",
              " '비추어 보아',\n",
              " '비하면',\n",
              " '뿐만 아니라',\n",
              " '뿐만아니라',\n",
              " '뿐이다',\n",
              " '삐걱',\n",
              " '삐걱거리다',\n",
              " '사',\n",
              " '삼',\n",
              " '상대적으로 말하자면',\n",
              " '생각한대로',\n",
              " '설령',\n",
              " '설마',\n",
              " '설사',\n",
              " '셋',\n",
              " '소생',\n",
              " '소인',\n",
              " '솨',\n",
              " '쉿',\n",
              " '습니까',\n",
              " '습니다',\n",
              " '시각',\n",
              " '시간',\n",
              " '시작하여',\n",
              " '시초에',\n",
              " '시키다',\n",
              " '실로',\n",
              " '심지어',\n",
              " '아',\n",
              " '아니',\n",
              " '아니나다를가',\n",
              " '아니라면',\n",
              " '아니면',\n",
              " '아니었다면',\n",
              " '아래윗',\n",
              " '아무거나',\n",
              " '아무도',\n",
              " '아야',\n",
              " '아울러',\n",
              " '아이',\n",
              " '아이고',\n",
              " '아이구',\n",
              " '아이야',\n",
              " '아이쿠',\n",
              " '아하',\n",
              " '아홉',\n",
              " '안 그러면',\n",
              " '않기 위하여',\n",
              " '않기 위해서',\n",
              " '알 수 있다',\n",
              " '알았어',\n",
              " '앗',\n",
              " '앞에서',\n",
              " '앞의것',\n",
              " '야',\n",
              " '약간',\n",
              " '양자',\n",
              " '어',\n",
              " '어기여차',\n",
              " '어느',\n",
              " '어느 년도',\n",
              " '어느것',\n",
              " '어느곳',\n",
              " '어느때',\n",
              " '어느쪽',\n",
              " '어느해',\n",
              " '어디',\n",
              " '어때',\n",
              " '어떠한',\n",
              " '어떤',\n",
              " '어떤것',\n",
              " '어떤것들',\n",
              " '어떻게',\n",
              " '어떻해',\n",
              " '어이',\n",
              " '어째서',\n",
              " '어쨋든',\n",
              " '어쩔수 없다',\n",
              " '어찌',\n",
              " '어찌됏든',\n",
              " '어찌됏어',\n",
              " '어찌하든지',\n",
              " '어찌하여',\n",
              " '언제',\n",
              " '언젠가',\n",
              " '얼마',\n",
              " '얼마 안 되는 것',\n",
              " '얼마간',\n",
              " '얼마나',\n",
              " '얼마든지',\n",
              " '얼마만큼',\n",
              " '얼마큼',\n",
              " '엉엉',\n",
              " '에',\n",
              " '에 가서',\n",
              " '에 달려 있다',\n",
              " '에 대해',\n",
              " '에 있다',\n",
              " '에 한하다',\n",
              " '에게',\n",
              " '에서',\n",
              " '여',\n",
              " '여기',\n",
              " '여덟',\n",
              " '여러분',\n",
              " '여보시오',\n",
              " '여부',\n",
              " '여섯',\n",
              " '여전히',\n",
              " '여차',\n",
              " '연관되다',\n",
              " '연이서',\n",
              " '영',\n",
              " '영차',\n",
              " '옆사람',\n",
              " '예',\n",
              " '예를 들면',\n",
              " '예를 들자면',\n",
              " '예컨대',\n",
              " '예하면',\n",
              " '오',\n",
              " '오로지',\n",
              " '오르다',\n",
              " '오자마자',\n",
              " '오직',\n",
              " '오호',\n",
              " '오히려',\n",
              " '와',\n",
              " '와 같은 사람들',\n",
              " '와르르',\n",
              " '와아',\n",
              " '왜',\n",
              " '왜냐하면',\n",
              " '외에도',\n",
              " '요만큼',\n",
              " '요만한 것',\n",
              " '요만한걸',\n",
              " '요컨대',\n",
              " '우르르',\n",
              " '우리',\n",
              " '우리들',\n",
              " '우선',\n",
              " '우에 종합한것과같이',\n",
              " '운운',\n",
              " '월',\n",
              " '위에서 서술한바와같이',\n",
              " '위하여',\n",
              " '위해서',\n",
              " '윙윙',\n",
              " '육',\n",
              " '으로',\n",
              " '으로 인하여',\n",
              " '으로서',\n",
              " '으로써',\n",
              " '을',\n",
              " '응',\n",
              " '응당',\n",
              " '의',\n",
              " '의거하여',\n",
              " '의지하여',\n",
              " '의해',\n",
              " '의해되다',\n",
              " '의해서',\n",
              " '이',\n",
              " '이 되다',\n",
              " '이 때문에',\n",
              " '이 밖에',\n",
              " '이 외에',\n",
              " '이 정도의',\n",
              " '이것',\n",
              " '이곳',\n",
              " '이때',\n",
              " '이라면',\n",
              " '이래',\n",
              " '이러이러하다',\n",
              " '이러한',\n",
              " '이런',\n",
              " '이럴정도로',\n",
              " '이렇게 많은 것',\n",
              " '이렇게되면',\n",
              " '이렇게말하자면',\n",
              " '이렇구나',\n",
              " '이로 인하여',\n",
              " '이르기까지',\n",
              " '이리하여',\n",
              " '이만큼',\n",
              " '이번',\n",
              " '이봐',\n",
              " '이상',\n",
              " '이어서',\n",
              " '이었다',\n",
              " '이와 같다',\n",
              " '이와 같은',\n",
              " '이와 반대로',\n",
              " '이와같다면',\n",
              " '이외에도',\n",
              " '이용하여',\n",
              " '이유만으로',\n",
              " '이젠',\n",
              " '이지만',\n",
              " '이쪽',\n",
              " '이천구',\n",
              " '이천육',\n",
              " '이천칠',\n",
              " '이천팔',\n",
              " '인 듯하다',\n",
              " '인젠',\n",
              " '일',\n",
              " '일것이다',\n",
              " '일곱',\n",
              " '일단',\n",
              " '일때',\n",
              " '일반적으로',\n",
              " '일지라도',\n",
              " '임에 틀림없다',\n",
              " '입각하여',\n",
              " '입장에서',\n",
              " '잇따라',\n",
              " '있다',\n",
              " '자',\n",
              " '자기',\n",
              " '자기집',\n",
              " '자마자',\n",
              " '자신',\n",
              " '잠깐',\n",
              " '잠시',\n",
              " '저',\n",
              " '저것',\n",
              " '저것만큼',\n",
              " '저기',\n",
              " '저쪽',\n",
              " '저희',\n",
              " '전부',\n",
              " '전자',\n",
              " '전후',\n",
              " '점에서 보아',\n",
              " '정도에 이르다',\n",
              " '제',\n",
              " '제각기',\n",
              " '제외하고',\n",
              " '조금',\n",
              " '조차',\n",
              " '조차도',\n",
              " '졸졸',\n",
              " '좀',\n",
              " '좋아',\n",
              " '좍좍',\n",
              " '주룩주룩',\n",
              " '주저하지 않고',\n",
              " '줄은 몰랏다',\n",
              " '줄은모른다',\n",
              " '중에서',\n",
              " '중의하나',\n",
              " '즈음하여',\n",
              " '즉',\n",
              " '즉시',\n",
              " '지든지',\n",
              " '지만',\n",
              " '지말고',\n",
              " '진짜로',\n",
              " '쪽으로',\n",
              " '차라리',\n",
              " '참',\n",
              " '참나',\n",
              " '첫번째로',\n",
              " '쳇',\n",
              " '총적으로',\n",
              " '총적으로 말하면',\n",
              " '총적으로 보면',\n",
              " '칠',\n",
              " '콸콸',\n",
              " '쾅쾅',\n",
              " '쿵',\n",
              " '타다',\n",
              " '타인',\n",
              " '탕탕',\n",
              " '토하다',\n",
              " '통하여',\n",
              " '툭',\n",
              " '퉤',\n",
              " '틈타',\n",
              " '팍',\n",
              " '팔',\n",
              " '퍽',\n",
              " '펄렁',\n",
              " '하',\n",
              " '하게될것이다',\n",
              " '하게하다',\n",
              " '하겠는가',\n",
              " '하고 있다',\n",
              " '하고있었다',\n",
              " '하곤하였다',\n",
              " '하구나',\n",
              " '하기 때문에',\n",
              " '하기 위하여',\n",
              " '하기는한데',\n",
              " '하기만 하면',\n",
              " '하기보다는',\n",
              " '하기에',\n",
              " '하나',\n",
              " '하느니',\n",
              " '하는 김에',\n",
              " '하는 편이 낫다',\n",
              " '하는것도',\n",
              " '하는것만 못하다',\n",
              " '하는것이 낫다',\n",
              " '하는바',\n",
              " '하더라도',\n",
              " '하도다',\n",
              " '하도록시키다',\n",
              " '하도록하다',\n",
              " '하든지',\n",
              " '하려고하다',\n",
              " '하마터면',\n",
              " '하면 할수록',\n",
              " '하면된다',\n",
              " '하면서',\n",
              " '하물며',\n",
              " '하여금',\n",
              " '하여야',\n",
              " '하자마자',\n",
              " '하지 않는다면',\n",
              " '하지 않도록',\n",
              " '하지마',\n",
              " '하지마라',\n",
              " '하지만',\n",
              " '하하',\n",
              " '한 까닭에',\n",
              " '한 이유는',\n",
              " '한 후',\n",
              " '한다면',\n",
              " '한다면 몰라도',\n",
              " '한데',\n",
              " '한마디',\n",
              " '한적이있다',\n",
              " '한켠으로는',\n",
              " '한항목',\n",
              " '할 따름이다',\n",
              " '할 생각이다',\n",
              " '할 줄 안다',\n",
              " '할 지경이다',\n",
              " '할 힘이 있다',\n",
              " '할때',\n",
              " '할만하다',\n",
              " '할망정',\n",
              " '할뿐',\n",
              " '할수있다',\n",
              " '할수있어',\n",
              " '할줄알다',\n",
              " '할지라도',\n",
              " '할지언정',\n",
              " '함께',\n",
              " '해도된다',\n",
              " '해도좋다',\n",
              " '해봐요',\n",
              " '해서는 안된다',\n",
              " '해야한다',\n",
              " '해요',\n",
              " '했어요',\n",
              " '향하다',\n",
              " '향하여',\n",
              " '향해서',\n",
              " '허',\n",
              " '허걱',\n",
              " '허허',\n",
              " '헉',\n",
              " '헉헉',\n",
              " '헐떡헐떡',\n",
              " '형식으로 쓰여',\n",
              " '혹시',\n",
              " '혹은',\n",
              " '혼자',\n",
              " '훨씬',\n",
              " '휘익',\n",
              " '휴',\n",
              " '흐흐',\n",
              " '흥',\n",
              " '힘입어'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "stop_words = set(stop_words)\n",
        "stop_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t76Q1pQ4WSk"
      },
      "source": [
        "### 단어 사전 구축 함수 구현 \n",
        "- 문서 리스트를 입력 받아 사전을 생성하는 `make_vocab()` 함수를 구현하라.\n",
        "- 함수 정의\n",
        "    - 입력 매개변수\n",
        "        - docs : 문서 리스트\n",
        "        - min_count : 최소 단어 등장 빈도수 (단어 빈도가 `min_count` 미만인 단어는 사전에 포함하지 않음)\n",
        "    - 조건\n",
        "        - 문서 길이 제한\n",
        "            - 단어 개수가 3개 이하인 문서는 처리하지 않음. (skip)\n",
        "        - 사전에 포함되는 단어 빈도수 제한\n",
        "            - 단어가 빈도가 `min_count` 미만은 단어는 사전에 포함하지 않음.\n",
        "        - 불용어 제거 \n",
        "            - 불용어 리스트에 포함된 단어는 제거 \n",
        "    - 반환값 \n",
        "        - word2count : 단어별 빈도 사전 (key: 단어, value: 등장 횟수)\n",
        "        - wid2word : 단어별 인덱스(wid) 사전 (key: 단어 인덱스(int), value: 단어)\n",
        "        - word2wid : 인덱스(wid)별 단어 사전 (key: 단어, value: 단어 인덱스(int))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:01.637431Z",
          "start_time": "2022-02-19T14:32:56.730711Z"
        },
        "id": "xkjqztIA4WSl"
      },
      "outputs": [],
      "source": [
        "# 코퍼스 로드\n",
        "with open(\"/content/drive/MyDrive/[Wanted]week3-1/tokenized/wiki_ko_mecab.txt\",'r') as f:\n",
        "  docs = f.read().split(\"\\n\") # delimiter : enter(\\n)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:03.423002Z",
          "start_time": "2022-02-19T14:33:03.419818Z"
        },
        "id": "WAKB6bbt4WSl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21c54930-392d-4ffc-b0a3-341fc127744a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# wiki documents: 311,238\n"
          ]
        }
      ],
      "source": [
        "print(f\"# wiki documents: {len(docs):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:04.016885Z",
          "start_time": "2022-02-19T14:33:03.962269Z"
        },
        "id": "-OI1MCXv4WSl"
      },
      "outputs": [],
      "source": [
        "# 문서 개수를 500개로 줄임\n",
        "docs=random.sample(docs,500)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"# wiki documents: {len(docs):,}\")"
      ],
      "metadata": {
        "id": "mP5wGu9YwDUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddac190a-d8ef-4c2a-bb0a-b0c4732540a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# wiki documents: 500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[11]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "id": "BH5dDG3VOOSj",
        "outputId": "5053abbd-9c7a-47f9-a9c2-d9cad24b7028"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'남양 초등 학교 백금 분교 는 충청남도 청양군 남 양면 백 금리 2 7 3 - 2 에 있 었 던 공립 초등 학교 이 다 . * 1 9 6 4 . 9 . 1 . 설립 인가 ( 5 교실 ) * 1 9 6 4 . 1 0 . 1 . 개교 ( 8 학급 인가 ) * 1 9 7 0 . 3 . 1 . 1 2 학급 인가 * 1 9 7 7 . 3 . 1 . 1 1 학급 인가 * 1 9 8 6 . 1 . 1 . 벽지 학교 지정 * 1 9 8 6 . 3 . 1 . 6 학급 인가 * 1 9 8 7 . 1 . 5 . 시 , 군 , 구 , 읍 , 면 의 관할 구역 변경 에 따른 학교 명칭 및 위치 ( 주소 ) 변경 * 1 9 9 3 . 3 . 1 . 5 학급 인가 * 1 9 9 8 . 3 . 1 . 3 학급 인가 * 2 0 0 6 . 3 . 1 . 남양 초등 학교 백금 분교장 ( 2 학급 인가 ) * 2 0 0 6 . 9 . 1 . 남양 초등 학교 로 통합 백금 은 백 월산 의 기점 과 종점 으로 이용 되 는 마을 금곡 은 ‘ 거문고 골짜기 ’ 라는 뜻 으로 , 마을 지형 이 거문고 처럼 생겼 다 해서 붙여진 이름 으로서 마을 이름 을 따 서 백금 초등 학교 라 지 음 분류 : 남양 초등 학교 ( 충남 ) 분류 : 1 9 6 4 년 개교 분류 : 2 0 0 6 년 폐교 분류 : 대한민국 의 없 어 진 초등 학교 분류 : 청양군 의 학교 분류 : 초등 학교 분교'"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:26.392627Z",
          "start_time": "2022-02-19T14:33:26.382358Z"
        },
        "id": "aJaEAVm9sAqv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "3601b8f6-410d-4ec4-f0aa-6b0bb3bc22f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'남양 초등 학교 백금 분교 는 충청남도 청양군 남 양면 백 금리 에 있 었 던 공립 초등 학교 이 다 설립 인가 교실 개교 학급 인가 학급 인가 학급 인가 벽지 학교 지정 학급 인가 시 군 구 읍 면 의 관할 구역 변경 에 따른 학교 명칭 및 위치 주소 변경 학급 인가 학급 인가 남양 초등 학교 백금 분교장 학급 인가 남양 초등 학교 로 통합 백금 은 백 월산 의 기점 과 종점 으로 이용 되 는 마을 금곡 은 거문고 골짜기 라는 뜻 으로 마을 지형 이 거문고 처럼 생겼 다 해서 붙여진 이름 으로서 마을 이름 을 따 서 백금 초등 학교 라 지 음 분류 남양 초등 학교 충남 분류 년 개교 분류 년 폐교 분류 대한민국 의 없 어 진 초등 학교 분류 청양군 의 학교 분류 초등 학교 분교'"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# 문서 내 숫자, 영어 대소문자, 특수문자를 제거 (re package 사용)\n",
        "import re\n",
        "def preprocess_line(line):\n",
        "    line = re.sub(r'[^가-힣]+', r\" \", line) # 숫자 제거\n",
        "    \n",
        "    # line = re.sub(r\"[0-9]+\", r\" \", line) # 숫자 제거\n",
        "    # line = re.sub(r\"[a-zA-Z]+\", r\" \", line) # 알파벳 대소문자 제거\n",
        "    # line = re.sub(r\"[~!@#$%<>^&*()-=+_`\\\"?》《–・〈〉“”‘’·…■]+\", r\" \", line) # 특수문자 제거\n",
        "    # line = re.sub(r\"[一-龥]+\", r\" \", line) # 한자 제거\n",
        "\n",
        "    # line = re.sub(r\"[\\*]+\", r\" \", line) # 기타 \\로시작하는 알수 없는 토큰 제거\n",
        "    \n",
        "    line = re.sub(r'[\" \"]+', r\" \",line) # 공백이 길경우 하나로 줄이기\n",
        "    # line = line.strip()\n",
        "    return line\n",
        "preprocess_line(docs[11])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs = list(map(preprocess_line,docs))\n",
        "# docs[11]"
      ],
      "metadata": {
        "id": "5tPunS-U6yFP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Check : {docs[0][:1000]}\")"
      ],
      "metadata": {
        "id": "sytiSICawMk5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f5629bf-7c9d-47f7-b4a0-41db65094bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Check : 남모 공주 는 신라 의 공주 왕족 으로 법흥왕 과 보과 공주 부여 씨 의 딸 이 며 백제 동성왕 의 외손녀 였 다 경쟁자 인 준정 과 함께 신라 의 초대 여성 원화 화랑 였 다 그 가 준정 에게 암살 당한 것 을 계기 로 화랑 은 여성 이 아닌 남성 미소년 으로 선발 하 게 되 었 다 신라 진흥왕 에게 는 사촌 누나 이 자 이모 가 된다 신라 의 청소년 조직 이 었 던 화랑도 는 처음 에 는 남모 준정 두 미녀 를 뽑 아 이 를 원화 라 했으며 이 들 주위 에 는 여 명 의 무리 를 따르 게 하 였 다 그러나 준정 과 남모 는 서로 최고 가 되 고자 시기 하 였 다 준정 은 박영실 을 섬겼 는데 지소태후 는 자신 의 두 번 째 남편 이 기 도 한 그 를 싫어해서 준정 의 원화 를 없애 고 낭도 가 부족 한 남모 에게 위화랑 의 낭도 를 더 해 주 었 다 그 뒤 남모 는 준정 의 초대 로 그 의 집 에 갔 다가 억지로 권하 는 술 을 받아마시 고 취한 뒤 준정 에 의해 강물 에 던져져 살해 되 었 다 이 일 이 발각 돼 준 정도 사형 에 처해지 고 나라 에서 는 귀족 출신 의 잘 생기 고 품행 이 곧 은 남자 를 뽑 아 곱 게 단장 한 후 이 를 화랑 이 라 칭하 고 받들 게 하 였 다 부왕 신라 제 대 국왕 법흥왕 모후 보과 공주 부여 씨 공주 남모 공주 외조부 백제 제 대 국왕 동성왕 외조모 신라 이찬 비지 의 딸 화랑전사 마루 년 배우 박효빈 신라 법흥왕 백제 동성왕 준정 화랑 분류 년 죽음 분류 신라 의 왕녀 분류 신라 의 왕족 분류 화랑 분류 암살 된 사람 분류 독살 된 사람 분류 법흥왕\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:27.904880Z",
          "start_time": "2022-02-19T14:33:27.899620Z"
        },
        "id": "OAkkQsvO4WSl"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def make_vocab(docs:List[str], min_count:int):\n",
        "    \"\"\"\n",
        "    'docs'문서 리스트를 입력 받아 단어 사전을 생성.\n",
        "    \n",
        "    return \n",
        "        - word2count : 단어별 빈도 사전\n",
        "        - wid2word : 단어별 인덱스(wid) 사전 \n",
        "        - word2wid : 인덱스(wid)별 단어 사전\n",
        "    \"\"\"\n",
        "\n",
        "    word2count = dict()\n",
        "    word2id = dict()\n",
        "    id2word = dict()\n",
        "\n",
        "    \n",
        "    for doc in tqdm(docs):\n",
        "        word_list = doc.split()\n",
        "        # 1. 문서 길이 제한\n",
        "        if len(word_list)<4:\n",
        "          continue;\n",
        "        # 2. 임시 딕셔너리(_word2count)에 단어별 등장 빈도 기록\n",
        "\n",
        "        _word2count=dict(Counter(word_list))\n",
        "        word2count={**word2count, **{key:val for key,val in _word2count.items() if val>=min_count}}\n",
        "        # 3. 불용어 제거\n",
        "        dickeys=word2count.keys()\n",
        "        for word in stop_words:\n",
        "          if word in set(dickeys):\n",
        "            word2count.pop(word, None)\n",
        "        # 4. 토큰 최소 빈도를 만족하는 토큰만 사전에 추가\n",
        "        for word in word2count:\n",
        "          if word not in set(word2id.keys()):\n",
        "            if word2id!={}:\n",
        "              word2id[word] = max(word2id.values()) +1 \n",
        "            else:\n",
        "              word2id[word] = 1\n",
        "        # id2word\n",
        "    id2word={val:key for key,val in word2id.items()}\n",
        "    \n",
        "    return word2count, word2id, id2word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:30.157872Z",
          "start_time": "2022-02-19T14:33:28.473330Z"
        },
        "id": "ieS5SiQx4WSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03352db5-c455-47f1-af37-51adddeae0cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:21<00:00,  6.15it/s]\n"
          ]
        }
      ],
      "source": [
        "word2count, word2id, id2word = make_vocab(docs, min_count=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2count"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxjxoj32AqnW",
        "outputId": "c44e9fbe-1a77-4eb0-db24-78ddb90fea33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'남모': 6,\n",
              " '공주': 34,\n",
              " '는': 14,\n",
              " '신라': 8,\n",
              " '였': 5,\n",
              " '다': 15,\n",
              " '준정': 9,\n",
              " '화랑': 155,\n",
              " '고': 6,\n",
              " '분류': 17,\n",
              " '헝가리': 7,\n",
              " '펜싱': 7,\n",
              " '선수': 14,\n",
              " '하계': 6,\n",
              " '올림픽': 9,\n",
              " '레이다': 18,\n",
              " '실전': 5,\n",
              " '배치': 5,\n",
              " '은': 5,\n",
              " '목록': 13,\n",
              " '강': 5,\n",
              " '목': 27,\n",
              " '아과': 17,\n",
              " '붉': 10,\n",
              " '디': 5,\n",
              " '움': 5,\n",
              " '풀': 12,\n",
              " '엘라': 11,\n",
              " '라': 7,\n",
              " '수': 6,\n",
              " '있': 6,\n",
              " '파일': 6,\n",
              " '며': 18,\n",
              " '시동': 6,\n",
              " '실행': 8,\n",
              " '할': 9,\n",
              " '명령어': 8,\n",
              " '윈도': 10,\n",
              " '버전': 6,\n",
              " '포함': 9,\n",
              " '한': 6,\n",
              " '도스': 15,\n",
              " '컴퓨터': 10,\n",
              " '변수': 5,\n",
              " '사용': 5,\n",
              " '지': 5,\n",
              " '되': 12,\n",
              " '된다': 10,\n",
              " '키보드': 5,\n",
              " '설정': 10,\n",
              " '드라이버': 9,\n",
              " '줄': 5,\n",
              " '기본': 6,\n",
              " '적': 5,\n",
              " '구성': 6,\n",
              " '청안': 6,\n",
              " '학교': 5,\n",
              " '선남': 5,\n",
              " '초등': 6,\n",
              " '라마': 8,\n",
              " '장갑순양함': 6,\n",
              " '었': 10,\n",
              " '함': 9,\n",
              " '했': 5,\n",
              " '웹툰': 8,\n",
              " '연재': 9,\n",
              " '윤인완': 7,\n",
              " '게': 5,\n",
              " '멀티': 5,\n",
              " '버스': 11,\n",
              " '인류': 6,\n",
              " '대': 5,\n",
              " '앤': 40,\n",
              " '불린': 32,\n",
              " '헨리': 9,\n",
              " '세': 7,\n",
              " '엘리자베스': 5,\n",
              " '결혼': 6,\n",
              " '잉글랜드': 5,\n",
              " '후': 5,\n",
              " '받': 5,\n",
              " '던': 8,\n",
              " '왕비': 9,\n",
              " '캐서린': 8,\n",
              " '그녀': 13,\n",
              " '메리': 5,\n",
              " '프랑스': 8,\n",
              " '았': 5,\n",
              " '시녀': 5,\n",
              " '도': 7,\n",
              " '공작': 5,\n",
              " '사람': 5,\n",
              " '왕': 5,\n",
              " '참수': 5,\n",
              " '인가': 5,\n",
              " '학급': 9,\n",
              " '코지마': 6,\n",
              " '하루카': 7,\n",
              " '유이': 5,\n",
              " '팀': 6,\n",
              " '영화': 9,\n",
              " '파우': 8,\n",
              " '스타': 6,\n",
              " '로마': 8,\n",
              " '콘스탄티누스': 12,\n",
              " '뒤': 5,\n",
              " '대머리': 8,\n",
              " '우': 5,\n",
              " '아카리': 8,\n",
              " '아프리카': 5,\n",
              " '본선': 9,\n",
              " '진출': 5,\n",
              " '승점': 8,\n",
              " '경기': 27,\n",
              " '승': 7,\n",
              " '무': 6,\n",
              " '패': 7,\n",
              " '득': 7,\n",
              " '실': 7,\n",
              " '차': 6,\n",
              " '기권': 6,\n",
              " '커피': 36,\n",
              " '드립': 14,\n",
              " '물': 6,\n",
              " '필터': 24,\n",
              " '통과': 6,\n",
              " '라고': 8,\n",
              " '종이': 16,\n",
              " '기': 5,\n",
              " '야쿠모': 5,\n",
              " '일본': 5,\n",
              " '해군': 7,\n",
              " '전쟁': 15,\n",
              " '된': 7,\n",
              " '해': 6,\n",
              " '주포': 5,\n",
              " '형': 5,\n",
              " '구경': 8,\n",
              " '포': 8,\n",
              " '앙각': 5,\n",
              " '축구': 17,\n",
              " '가제': 8,\n",
              " '바람': 8,\n",
              " '싱글': 8,\n",
              " '앨범': 5,\n",
              " '번': 7,\n",
              " '째': 7,\n",
              " '수록': 14,\n",
              " '곡': 7,\n",
              " '감독': 5,\n",
              " '아리': 9,\n",
              " '와라': 8,\n",
              " '노': 12,\n",
              " '나리': 20,\n",
              " '히라': 22,\n",
              " '시대': 5,\n",
              " '천황': 12,\n",
              " '친왕': 7,\n",
              " '면서': 5,\n",
              " '없': 7,\n",
              " '종': 11,\n",
              " '승진': 5,\n",
              " '인': 6,\n",
              " '와카': 15,\n",
              " '쓰': 10,\n",
              " '가인': 6,\n",
              " '고킨와카슈': 5,\n",
              " '집': 14,\n",
              " '이야기': 8,\n",
              " '씨': 5,\n",
              " '야간': 7,\n",
              " '시력': 5,\n",
              " '볼': 7,\n",
              " '스펙트럼': 7,\n",
              " '범위': 8,\n",
              " '인간': 6,\n",
              " '포테마요': 15,\n",
              " '만화': 6,\n",
              " '애니메이션': 6,\n",
              " '엔': 14,\n",
              " '성우': 5,\n",
              " '스나오': 18,\n",
              " '머리': 11,\n",
              " '같': 5,\n",
              " '곤': 5,\n",
              " '한다': 14,\n",
              " '클래스': 6,\n",
              " '메이트': 6,\n",
              " '미캉': 10,\n",
              " '이름': 11,\n",
              " '츄': 6,\n",
              " '코': 5,\n",
              " '쿄': 6,\n",
              " '음력': 6,\n",
              " '코오롱': 61,\n",
              " '그룹': 5,\n",
              " '주': 5,\n",
              " '지주회사': 7,\n",
              " '기업': 11,\n",
              " '제약': 6,\n",
              " '사업': 20,\n",
              " '보유': 6,\n",
              " '계열사': 8,\n",
              " '약': 5,\n",
              " '인더스': 6,\n",
              " '트리': 15,\n",
              " '수익': 6,\n",
              " '글로벌': 5,\n",
              " '생명': 7,\n",
              " '리조트': 5,\n",
              " '연구소': 6,\n",
              " '매각': 5,\n",
              " '산': 6,\n",
              " '이사벨라': 10,\n",
              " '섬': 7,\n",
              " '갈라파고스': 16,\n",
              " '가장': 6,\n",
              " '칸': 6,\n",
              " '톤': 7,\n",
              " '화산': 11,\n",
              " '여자': 5,\n",
              " '오쿠노': 13,\n",
              " '진주': 8,\n",
              " '옥스포드': 7,\n",
              " '레고': 15,\n",
              " '대한민국': 5,\n",
              " '블록': 10,\n",
              " '완구': 6,\n",
              " '메가': 5,\n",
              " '블럭': 5,\n",
              " '소재': 5,\n",
              " '크리스탈': 8,\n",
              " '기능': 5,\n",
              " '이성주': 6,\n",
              " '자동차': 6,\n",
              " '권은희': 12,\n",
              " '경찰': 16,\n",
              " '사법': 5,\n",
              " '변호사': 5,\n",
              " '특별': 8,\n",
              " '선거': 5,\n",
              " '국회의원': 5,\n",
              " '새': 35,\n",
              " '정치': 5,\n",
              " '민주': 5,\n",
              " '연합': 9,\n",
              " '대학교': 5,\n",
              " '경찰서': 6,\n",
              " '과장': 5,\n",
              " '서울': 6,\n",
              " '수서': 7,\n",
              " '사건': 8,\n",
              " '라는': 5,\n",
              " '수사': 5,\n",
              " '당시': 7,\n",
              " '경찰청': 6,\n",
              " '중': 5,\n",
              " '권': 7,\n",
              " '진술': 5,\n",
              " '사실': 6,\n",
              " '위증': 7,\n",
              " '검찰': 8,\n",
              " '다는': 6,\n",
              " '대해': 10,\n",
              " '다고': 13,\n",
              " '관련': 5,\n",
              " '주장': 9,\n",
              " '분석': 6,\n",
              " '개': 5,\n",
              " '진실': 6,\n",
              " '성': 9,\n",
              " '논문': 18,\n",
              " '표절': 6,\n",
              " '각주': 5,\n",
              " '위원회': 5,\n",
              " '광주': 5,\n",
              " '국민': 8,\n",
              " '당': 6,\n",
              " '국회': 12,\n",
              " '후반기': 6,\n",
              " '위원': 5,\n",
              " '간사': 7,\n",
              " '바른': 5,\n",
              " '미래': 10,\n",
              " '동문': 5,\n",
              " '인텔': 50,\n",
              " '테라': 5,\n",
              " '프로세서': 13,\n",
              " '드라마': 8,\n",
              " '방송': 6,\n",
              " '작곡가': 5,\n",
              " '러셀': 6,\n",
              " '역': 38,\n",
              " '래피드': 39,\n",
              " '마이너': 38,\n",
              " '데이터': 7,\n",
              " '소프트웨어': 5,\n",
              " '프로그램': 14,\n",
              " '회사': 5,\n",
              " '개발': 8,\n",
              " '마이닝': 5,\n",
              " '제공': 5,\n",
              " '처리': 5,\n",
              " '지원': 20,\n",
              " '만': 8,\n",
              " '다양': 5,\n",
              " '도록': 5,\n",
              " '가수': 7,\n",
              " '사랑': 10,\n",
              " '겨울': 5,\n",
              " '수다': 14,\n",
              " '불후': 10,\n",
              " '명곡': 10,\n",
              " '전설': 13,\n",
              " '노래': 10,\n",
              " '음악': 9,\n",
              " '쇼': 6,\n",
              " '정규': 13,\n",
              " '발매': 6,\n",
              " '듀엣': 7,\n",
              " '월화드라마': 6,\n",
              " '수목드라마': 5,\n",
              " '회': 20,\n",
              " '국민주의': 21,\n",
              " '자유': 10,\n",
              " '사회': 28,\n",
              " '리그': 6,\n",
              " '근대': 10,\n",
              " '피터': 7,\n",
              " '첸': 7,\n",
              " '모델': 5,\n",
              " '박사': 5,\n",
              " '교수': 9,\n",
              " '아나운서': 5,\n",
              " '트랜스포머': 5,\n",
              " '다카마쓰': 9,\n",
              " '방송국': 13,\n",
              " '가가와현': 5,\n",
              " '구역': 11,\n",
              " '라디오': 9,\n",
              " '텔레비전': 14,\n",
              " '오카야마': 5,\n",
              " '개시': 7,\n",
              " '계열': 15,\n",
              " '검사': 9,\n",
              " '수석': 5,\n",
              " '비서관': 5,\n",
              " '대구': 5,\n",
              " '중앙': 5,\n",
              " '대검찰청': 5,\n",
              " '부장': 8,\n",
              " '지방': 5,\n",
              " '검찰청': 13,\n",
              " '수부': 5,\n",
              " '재직': 5,\n",
              " '멤버': 5,\n",
              " '이탈리아': 11,\n",
              " '우주': 6,\n",
              " '온주쿠': 6,\n",
              " '철도역': 5,\n",
              " '시': 11,\n",
              " '두산': 9,\n",
              " '베어스': 6,\n",
              " '야구': 6,\n",
              " '이후': 7,\n",
              " '배명': 7,\n",
              " '고등학교': 10,\n",
              " '투수': 6,\n",
              " '프로': 13,\n",
              " '시즌': 6,\n",
              " '상대': 7,\n",
              " '홈런': 12,\n",
              " '기록': 6,\n",
              " '통산': 7,\n",
              " '선언': 5,\n",
              " '은퇴': 6,\n",
              " '전처': 6,\n",
              " '이혼': 7,\n",
              " '아시안': 9,\n",
              " '게임': 5,\n",
              " '타점': 9,\n",
              " '역대': 5,\n",
              " '개인': 23,\n",
              " '메달리스트': 5,\n",
              " '참가': 5,\n",
              " '치카': 5,\n",
              " '미국': 21,\n",
              " '독립': 5,\n",
              " '체로키': 25,\n",
              " '족': 10,\n",
              " '지도자': 5,\n",
              " '드래': 10,\n",
              " '깅': 10,\n",
              " '카누': 10,\n",
              " '인디언': 42,\n",
              " '식민지': 29,\n",
              " '지역': 9,\n",
              " '버지니아': 9,\n",
              " '켄터키': 7,\n",
              " '테네시': 5,\n",
              " '부족': 11,\n",
              " '영국': 8,\n",
              " '힐': 73,\n",
              " '대한': 9,\n",
              " '캐롤라이나': 11,\n",
              " '협상': 5,\n",
              " '강과': 5,\n",
              " '노스': 13,\n",
              " '로버트슨': 6,\n",
              " '정착': 9,\n",
              " '공격': 20,\n",
              " '타': 5,\n",
              " '쿨라': 6,\n",
              " '응진': 8,\n",
              " '탱': 10,\n",
              " '나한도': 5,\n",
              " '제석': 5,\n",
              " '폭': 5,\n",
              " '존자': 6,\n",
              " '후지': 22,\n",
              " '또는': 9,\n",
              " '젠더': 63,\n",
              " '남성': 16,\n",
              " '여성': 11,\n",
              " '단어': 17,\n",
              " '특정': 5,\n",
              " '구분': 10,\n",
              " '쓰이': 5,\n",
              " '생물학': 12,\n",
              " '성별': 27,\n",
              " '역할': 18,\n",
              " '거나': 9,\n",
              " '용어': 5,\n",
              " '문화': 6,\n",
              " '는데': 17,\n",
              " '시작': 5,\n",
              " '이론': 10,\n",
              " '이러': 5,\n",
              " '의미': 5,\n",
              " '표현': 5,\n",
              " '말': 14,\n",
              " '행동': 5,\n",
              " '때문': 5,\n",
              " '어서': 7,\n",
              " '정체': 8,\n",
              " '신체': 5,\n",
              " '면': 23,\n",
              " '남녀': 5,\n",
              " '보다': 8,\n",
              " '정의': 5,\n",
              " '개념': 10,\n",
              " '집단': 8,\n",
              " '졸업식': 5,\n",
              " '졸업': 5,\n",
              " '생': 5,\n",
              " '총수': 5,\n",
              " '명': 11,\n",
              " '크리어': 20,\n",
              " '언어': 6,\n",
              " '캐나다': 5,\n",
              " '영어': 12,\n",
              " '처럼': 44,\n",
              " '발음': 73,\n",
              " '방언': 5,\n",
              " '자음': 7,\n",
              " '앞': 5,\n",
              " '점': 11,\n",
              " '경로': 18,\n",
              " '위상': 7,\n",
              " '곡선': 6,\n",
              " '화': 20,\n",
              " '속도': 5,\n",
              " '벡터': 25,\n",
              " '베이징': 19,\n",
              " '지하철': 6,\n",
              " '호': 11,\n",
              " '선': 13,\n",
              " '시즈먼': 6,\n",
              " '노선': 61,\n",
              " '먼': 8,\n",
              " '미츠키': 6,\n",
              " '미쓰키': 7,\n",
              " '다무라': 7,\n",
              " '정': 9,\n",
              " '출연': 8,\n",
              " '목표': 5,\n",
              " '성향': 179,\n",
              " '조직': 17,\n",
              " '도전': 147,\n",
              " '능력': 43,\n",
              " '려는': 5,\n",
              " '특성': 24,\n",
              " '초기': 5,\n",
              " '연구': 5,\n",
              " '그것': 9,\n",
              " '구성원': 12,\n",
              " '직무': 6,\n",
              " '평가': 6,\n",
              " '결과': 5,\n",
              " '나타날': 5,\n",
              " '중요': 17,\n",
              " '요인': 7,\n",
              " '학생': 19,\n",
              " '새로운': 10,\n",
              " '기술': 12,\n",
              " '지식': 7,\n",
              " '습득': 6,\n",
              " '고자': 5,\n",
              " '학습': 12,\n",
              " '높': 26,\n",
              " '가지': 6,\n",
              " '두': 7,\n",
              " '동시': 5,\n",
              " '서로': 5,\n",
              " '보': 10,\n",
              " '동기': 10,\n",
              " '부여': 7,\n",
              " '가진': 23,\n",
              " '달성': 5,\n",
              " '위해': 9,\n",
              " '않': 7,\n",
              " '알': 33,\n",
              " '과업': 32,\n",
              " '어려운': 7,\n",
              " '업무': 6,\n",
              " '수행': 20,\n",
              " '반면': 11,\n",
              " '통해': 8,\n",
              " '복잡': 6,\n",
              " '음': 5,\n",
              " '실패': 6,\n",
              " '가능': 5,\n",
              " '생각': 5,\n",
              " '초점': 8,\n",
              " '맞추': 6,\n",
              " '집중': 7,\n",
              " '환경': 11,\n",
              " '주어진': 10,\n",
              " '이해': 9,\n",
              " '시키': 6,\n",
              " '좋': 5,\n",
              " '싶': 5,\n",
              " '입증': 13,\n",
              " '내': 8,\n",
              " '낮': 5,\n",
              " '으며': 6,\n",
              " '만들': 5,\n",
              " '향상': 5,\n",
              " '데': 6,\n",
              " '회피': 16,\n",
              " '성과': 31,\n",
              " '부정': 6,\n",
              " '지향': 7,\n",
              " '될': 5,\n",
              " '상황': 17,\n",
              " '어야': 6,\n",
              " '상태': 11,\n",
              " '측면': 9,\n",
              " '구조': 7,\n",
              " '설명': 5,\n",
              " '성취': 12,\n",
              " '분야': 11,\n",
              " '자체': 5,\n",
              " '이나': 5,\n",
              " '영향': 8,\n",
              " '더': 5,\n",
              " '나아가': 7,\n",
              " '관계': 7,\n",
              " '발견': 7,\n",
              " '해야': 5,\n",
              " '보여': 5,\n",
              " '피드백': 16,\n",
              " '실수': 5,\n",
              " '한다는': 6,\n",
              " '교육': 9,\n",
              " '과정': 12,\n",
              " '활동': 5,\n",
              " '노력': 7,\n",
              " '긍정': 5,\n",
              " '수준': 5,\n",
              " '욕구': 7,\n",
              " '많': 5,\n",
              " '사이': 7,\n",
              " '메타분석': 5,\n",
              " '인지': 26,\n",
              " '상관': 6,\n",
              " '예측': 5,\n",
              " '연관': 7,\n",
              " '성격': 5,\n",
              " '경험': 5,\n",
              " '구체': 6,\n",
              " '필요': 6,\n",
              " '간': 10,\n",
              " '효능감': 14,\n",
              " '메타': 14,\n",
              " '의하': 5,\n",
              " '교사': 6,\n",
              " '수용': 5,\n",
              " '정렬': 8,\n",
              " '그래프': 8,\n",
              " '꼭짓점': 8,\n",
              " '과목': 5,\n",
              " '경혜': 22,\n",
              " '문종': 5,\n",
              " '왕후': 5,\n",
              " '딸': 5,\n",
              " '단종': 6,\n",
              " '세종': 6,\n",
              " '전': 6,\n",
              " '죽': 27,\n",
              " '정종': 9,\n",
              " '혼인': 22,\n",
              " '동생': 5,\n",
              " '세조': 5,\n",
              " '재위': 5,\n",
              " '숙부': 7,\n",
              " '조선': 5,\n",
              " '카카오': 8,\n",
              " '게임즈': 10,\n",
              " '엔진': 11,\n",
              " '퍼블리싱': 8,\n",
              " '계약': 6,\n",
              " '체결': 10,\n",
              " '억': 8,\n",
              " '원': 65,\n",
              " '투자': 5,\n",
              " '하코다테': 11,\n",
              " '홋카이도': 5,\n",
              " '철도': 5,\n",
              " '오타': 10,\n",
              " '루': 5,\n",
              " '삿포로': 9,\n",
              " '미자': 5,\n",
              " '아사히카와': 5,\n",
              " '구간': 18,\n",
              " '계': 6,\n",
              " '키': 5,\n",
              " '동차': 5,\n",
              " '군': 9,\n",
              " '나이': 5,\n",
              " '신모리야마': 6,\n",
              " '모리야마': 9,\n",
              " '나고야': 7,\n",
              " '주오': 6,\n",
              " '열차': 14,\n",
              " '승강장': 5,\n",
              " '컨테이너': 12,\n",
              " '화물': 6,\n",
              " '개업': 7,\n",
              " '설치': 5,\n",
              " '취급': 6,\n",
              " '모리': 5,\n",
              " '폐지': 5,\n",
              " '엠버': 5,\n",
              " '가온': 8,\n",
              " '차트': 6,\n",
              " '탄생불': 6,\n",
              " '도덕경': 15,\n",
              " '노자': 14,\n",
              " '만물': 8,\n",
              " '는다': 8,\n",
              " '장': 11,\n",
              " '사상': 9,\n",
              " '으나': 5,\n",
              " '판본': 7,\n",
              " '말기': 5,\n",
              " '왕필': 17,\n",
              " '본': 58,\n",
              " '추정': 6,\n",
              " '백서': 27,\n",
              " '여러': 6,\n",
              " '곽': 11,\n",
              " '많이': 7,\n",
              " '내용': 7,\n",
              " '비해': 5,\n",
              " '성립': 5,\n",
              " '공간': 18,\n",
              " '위': 6,\n",
              " '존재': 12,\n",
              " '대수': 21,\n",
              " '전파': 24,\n",
              " '천문대': 6,\n",
              " '한국': 5,\n",
              " '관측': 18,\n",
              " '망': 8,\n",
              " '망원경': 9,\n",
              " '운영': 5,\n",
              " '밴드': 5,\n",
              " '시스템': 7,\n",
              " '독일': 5,\n",
              " '체코슬로바키아': 6,\n",
              " '체코': 5,\n",
              " '마장': 22,\n",
              " '기신': 18,\n",
              " '슈퍼': 9,\n",
              " '로봇': 30,\n",
              " '대전': 6,\n",
              " '시리즈': 13,\n",
              " '등장': 10,\n",
              " '반프레스토': 5,\n",
              " '오리지널': 6,\n",
              " '작품': 21,\n",
              " '본작': 5,\n",
              " '캐릭터': 7,\n",
              " '소프트': 8,\n",
              " '윙': 5,\n",
              " '류네': 7,\n",
              " '세계': 12,\n",
              " '바스': 10,\n",
              " '터': 9,\n",
              " '랑': 91,\n",
              " '존': 6,\n",
              " '슈': 7,\n",
              " '슈우': 27,\n",
              " '대결': 5,\n",
              " '마사키': 31,\n",
              " '일행': 6,\n",
              " '기아스': 11,\n",
              " '신': 16,\n",
              " '랑그': 12,\n",
              " '란': 5,\n",
              " '파일럿': 9,\n",
              " '지상': 13,\n",
              " '동료': 6,\n",
              " '오졸': 5,\n",
              " '슈테': 9,\n",
              " '드니': 9,\n",
              " '어스': 8,\n",
              " '쫓': 6,\n",
              " '론도': 15,\n",
              " '벨': 15,\n",
              " '다시': 5,\n",
              " '크로스': 7,\n",
              " '단': 11,\n",
              " '평': 9,\n",
              " '태평동': 7,\n",
              " '진해': 5,\n",
              " '선교사': 12,\n",
              " '챔니스': 9,\n",
              " '주택': 8,\n",
              " '돈': 12,\n",
              " '곤살로': 52,\n",
              " '코르도바': 6,\n",
              " '스페인': 10,\n",
              " '세기': 7,\n",
              " '체리': 9,\n",
              " '뇰라': 7,\n",
              " '전투': 7,\n",
              " '화력': 5,\n",
              " '참호': 8,\n",
              " '백작': 5,\n",
              " '가문': 5,\n",
              " '카스티야': 5,\n",
              " '매우': 5,\n",
              " '이사벨': 12,\n",
              " '장교': 8,\n",
              " '병사': 8,\n",
              " '페르난도': 5,\n",
              " '패배': 7,\n",
              " '그라나다': 6,\n",
              " '나폴리': 8,\n",
              " '프랑스군': 20,\n",
              " '기병': 22,\n",
              " '스위스': 11,\n",
              " '창병': 13,\n",
              " '보병': 12,\n",
              " '숫자': 7,\n",
              " '돌격': 7,\n",
              " '시켰': 8,\n",
              " '군대': 5,\n",
              " '장비': 6,\n",
              " '중장기병': 7,\n",
              " '밀집': 7,\n",
              " '일반': 5,\n",
              " '공성전': 7,\n",
              " '대형': 8,\n",
              " '병': 6,\n",
              " '했으나': 5,\n",
              " '부대': 6,\n",
              " '대포': 6,\n",
              " '전축': 5,\n",
              " '루이': 5,\n",
              " '주말': 6,\n",
              " '러시아': 7,\n",
              " '연방': 6,\n",
              " '중화': 6,\n",
              " '인민공화국': 7,\n",
              " '중화민국': 6,\n",
              " '제국': 21,\n",
              " '소비에트': 5,\n",
              " '조약': 10,\n",
              " '중국': 6,\n",
              " '중소': 5,\n",
              " '소련': 9,\n",
              " '카': 6,\n",
              " '우산': 7,\n",
              " '틴': 8,\n",
              " '막': 7,\n",
              " '토마스': 105,\n",
              " '아퀴나스': 84,\n",
              " '신학자': 8,\n",
              " '철학자': 5,\n",
              " '자연': 6,\n",
              " '신학': 14,\n",
              " '가톨릭': 16,\n",
              " '철학': 6,\n",
              " '사망': 6,\n",
              " '살': 10,\n",
              " '으로부터': 7,\n",
              " '수도회': 11,\n",
              " '몬테': 5,\n",
              " '카시노': 5,\n",
              " '수도원': 6,\n",
              " '서': 7,\n",
              " '수도사': 31,\n",
              " '길': 13,\n",
              " '학문': 7,\n",
              " '학자': 11,\n",
              " '아리스토텔레스': 9,\n",
              " '도미니코': 18,\n",
              " '특히': 5,\n",
              " '삶': 9,\n",
              " '작용': 7,\n",
              " '가족': 5,\n",
              " '더불': 29,\n",
              " '파리': 5,\n",
              " '로서': 7,\n",
              " '모든': 6,\n",
              " '일화': 6,\n",
              " '성직자': 7,\n",
              " '알베르투스': 19,\n",
              " '는지': 5,\n",
              " '대학': 7,\n",
              " '강의': 8,\n",
              " '시기': 10,\n",
              " '벙어리': 6,\n",
              " '황소': 6,\n",
              " '입장': 6,\n",
              " '젊': 11,\n",
              " '총장': 10,\n",
              " '기독교': 5,\n",
              " '명제': 6,\n",
              " '주석': 28,\n",
              " '집필': 8,\n",
              " '취임': 6,\n",
              " '출신': 5,\n",
              " '가운데': 5,\n",
              " '확인': 11,\n",
              " '토론': 17,\n",
              " '오르비에토': 5,\n",
              " '완성': 9,\n",
              " '주해': 11,\n",
              " '저작': 7,\n",
              " '신학대전': 9,\n",
              " '영혼': 31,\n",
              " '론': 9,\n",
              " '관하': 14,\n",
              " '부': 5,\n",
              " '감각': 5,\n",
              " '미완성': 7,\n",
              " '성인': 5,\n",
              " '이성': 9,\n",
              " '본질': 6,\n",
              " '원리': 5,\n",
              " '덕': 5,\n",
              " '지정': 11,\n",
              " '일진회': 9,\n",
              " '남원': 5,\n",
              " '아들': 5,\n",
              " '남': 5,\n",
              " '후손': 6,\n",
              " '김': 14,\n",
              " '거주': 6,\n",
              " '손자': 14,\n",
              " '증손자': 13,\n",
              " '진': 14,\n",
              " '향': 7,\n",
              " '경기장': 5,\n",
              " '역전': 5,\n",
              " '아야사토': 5,\n",
              " '아이슬란드': 9,\n",
              " '다람쥐': 6,\n",
              " '키프로스': 19,\n",
              " '왕국': 9,\n",
              " '오스만': 15,\n",
              " '베네치아': 16,\n",
              " '공화국': 7,\n",
              " '자크': 6,\n",
              " '카트린느': 7,\n",
              " '아이브': 6,\n",
              " '애플': 16,\n",
              " '디자인': 9,\n",
              " '명단': 6,\n",
              " '전라남도': 12,\n",
              " '함경남도': 13,\n",
              " '중추원': 51,\n",
              " '도지사': 31,\n",
              " '조선총독부': 7,\n",
              " '사무관': 54,\n",
              " '강원도': 12,\n",
              " '경시': 15,\n",
              " '경상북도': 15,\n",
              " '평안남도': 15,\n",
              " '경기도': 5,\n",
              " '평안북도': 10,\n",
              " '함경북도': 13,\n",
              " '경상남도': 13,\n",
              " '황해도': 5,\n",
              " '전라북도': 17,\n",
              " '황': 8,\n",
              " '해도': 8,\n",
              " '충청북도': 13,\n",
              " '충청남도': 11,\n",
              " '교단': 5,\n",
              " '목사': 5,\n",
              " '안수': 6,\n",
              " '허용': 7,\n",
              " '예수교': 5,\n",
              " '장로회': 5,\n",
              " '국가': 5,\n",
              " '타이베이': 7,\n",
              " '인정': 8,\n",
              " '유엔': 6,\n",
              " '경우': 5,\n",
              " '기원전': 20,\n",
              " '상': 8,\n",
              " '나라': 6,\n",
              " '왕실': 5,\n",
              " '세력': 8,\n",
              " '통일': 17,\n",
              " '황제': 5,\n",
              " '멸망': 17,\n",
              " '다가': 7,\n",
              " '후한': 6,\n",
              " '세운': 9,\n",
              " '못하': 5,\n",
              " '계승': 7,\n",
              " '차지': 5,\n",
              " '서진': 6,\n",
              " '한족': 7,\n",
              " '화북': 5,\n",
              " '건국': 7,\n",
              " '국제': 10,\n",
              " '몽골': 5,\n",
              " '대륙': 7,\n",
              " '정책': 6,\n",
              " '타이완': 16,\n",
              " '정부': 5,\n",
              " '홍콩': 33,\n",
              " '마카오': 5,\n",
              " '매춘': 10,\n",
              " '대통령': 6,\n",
              " '김영삼': 31,\n",
              " '비판': 14,\n",
              " '합당': 6,\n",
              " '제기': 7,\n",
              " '자금': 13,\n",
              " '안기부': 11,\n",
              " '대선': 10,\n",
              " '후보': 5,\n",
              " '가오리': 7,\n",
              " '발': 6,\n",
              " '매일': 8,\n",
              " '타이틀': 15,\n",
              " '꽃': 11,\n",
              " '피': 13,\n",
              " '첫': 19,\n",
              " '걸음': 10,\n",
              " '오프닝': 8,\n",
              " '엔딩': 10,\n",
              " '테마': 17,\n",
              " '기라티나': 32,\n",
              " '포켓몬스터': 6,\n",
              " '디아루가': 12,\n",
              " '펄기아': 9,\n",
              " '포켓몬': 12,\n",
              " '어나더': 6,\n",
              " '폼': 8,\n",
              " '깨어진': 5,\n",
              " '오리진': 8,\n",
              " '드래곤': 5,\n",
              " '섀도': 5,\n",
              " '다이브': 5,\n",
              " '도감': 5,\n",
              " '잡': 8,\n",
              " '기둥': 6,\n",
              " '등록': 5,\n",
              " '극장판': 6,\n",
              " '프리드리히': 11,\n",
              " '팔츠': 6,\n",
              " '보헤미아': 10,\n",
              " '로키산': 9,\n",
              " '홍반': 9,\n",
              " '열': 7,\n",
              " '질병': 5,\n",
              " '진드기': 7,\n",
              " '감염병': 5,\n",
              " '한림': 5,\n",
              " '언론': 8,\n",
              " '정보': 5,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:30.754722Z",
          "start_time": "2022-02-19T14:33:30.752115Z"
        },
        "id": "cT1MRN1EJtx6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab4c6d14-e69f-4cd8-e6b7-011d2ab21548"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34,755\n"
          ]
        }
      ],
      "source": [
        "doc_len = sum(word2count.values()) # 문서 내 모든 단어의 개수 (단어별 등장 빈도의 총 합)\n",
        "print(f\"{doc_len:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:32.916830Z",
          "start_time": "2022-02-19T14:33:32.914355Z"
        },
        "id": "e_1MneB54WSm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26d65322-48bd-40b3-b817-39bec4992708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# unique word : 2,982\n"
          ]
        }
      ],
      "source": [
        "print(f\"# unique word : {len(word2id):,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHxtZqtk4WSm"
      },
      "source": [
        "### Dataset 클래스 구현\n",
        "- Skip-Gram 방식의 학습 데이터 셋(`Tuple(target_word, context_word)`)을 생성하는 `CustomDataset` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()` 함수) 입력 매개변수\n",
        "        - docs: 문서 리스트\n",
        "        - word2id: 단어별 인덱스(wid) 사전\n",
        "        - window_size: Skip-Gram의 윈도우 사이즈\n",
        "    - 메소드\n",
        "        - `make_pair()`\n",
        "            - 문서를 단어로 쪼개고, 사전에 존재하는 단어들만 단어 인덱스로 변경\n",
        "            - Skip-gram 방식의 `(target_word, context_word)` 페어(tuple)들을 `pairs` 리스트에 담아 반환\n",
        "        - `__len__()`\n",
        "            - `pairs` 리스트의 개수 반환\n",
        "        - `__getitem__(index)`\n",
        "            - `pairs` 리스트를 인덱싱\n",
        "    - 주의 사항\n",
        "        - `nn.Module`를 부모 클래스로 상속 받음 \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.111290Z",
          "start_time": "2022-02-19T14:33:38.104531Z"
        },
        "id": "UPiLcYCZ4WSm"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    \"\"\"\n",
        "    문서 리스트를 받아 skip-gram 방식의 (target_word, context_word) 데이터 셋을 생성\n",
        "    \"\"\"\n",
        "    def __init__(self, docs:List[str], word2id:Dict[str,int], window_size:int=5):\n",
        "        self.docs = docs\n",
        "        self.word2id = word2id\n",
        "        if window_size%2==1:\n",
        "          self.window_size = window_size\n",
        "        else :\n",
        "          print(\"window size is not proper. try again\")\n",
        "        assert window_size%2==1\n",
        "        self.pairs = self.make_pair()\n",
        "    \n",
        "    def make_pair(self):\n",
        "        \"\"\"\n",
        "        (target, context) 형식의 Skip-gram pair 데이터 셋 생성 \n",
        "        \"\"\"\n",
        "        W = self.window_size\n",
        "        pairs = []\n",
        "        for doc in self.docs:\n",
        "          doclist= doc.split()\n",
        "          doclist=list(map(lambda x: self.word2id.get(x),doclist)) # id값 mapping\n",
        "          doclist=[i for i in doclist if i] # remove None element \n",
        "          N=len(doclist)\n",
        "          \n",
        "          tups=[]\n",
        "          for i in range(N-W+1): # i~i+4 \n",
        "            nhalf = int((W//2)/2)\n",
        "            # mid= i+nhalf\n",
        "            # tup_front = [(doclist[i+j],doclist[mid]) for j in range(nhalf)]\n",
        "            # tup_back = [(doclist[(mid+1)+j],doclist[mid]) for j in range(nhalf)]\n",
        "            sublist= [(doclist[i+j],doclist[i+nhalf]) for j in range(W) if j!=nhalf]\n",
        "            tups.extend(sublist)\n",
        "          pairs.extend(tups)\n",
        "        return pairs\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx][0], self.pairs[idx][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.945361Z",
          "start_time": "2022-02-19T14:33:38.385577Z"
        },
        "id": "YntOw2q94WSm"
      },
      "outputs": [],
      "source": [
        "dataset = CustomDataset(docs, word2id, window_size=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:38.949614Z",
          "start_time": "2022-02-19T14:33:38.946663Z"
        },
        "id": "-RpNbAjk4WSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efd0f7e6-507e-425b-f76d-9ae379e6a08f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "531204"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:43.072635Z",
          "start_time": "2022-02-19T14:33:43.069526Z"
        },
        "id": "1FBwcL4H4WSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f8c8bf2-7ded-4488-a85d-e6b13f06e33b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:33:51.040595Z",
          "start_time": "2022-02-19T14:33:51.031473Z"
        },
        "id": "wTAwTjKk4WSn",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "373fa2e2-3d63-4863-84f1-baa35f84eedd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(남모, 공주)\n",
            "(는, 공주)\n",
            "(신라, 공주)\n",
            "(공주, 공주)\n",
            "(공주, 는)\n",
            "(신라, 는)\n",
            "(공주, 는)\n",
            "(공주, 는)\n",
            "(는, 신라)\n",
            "(공주, 신라)\n",
            "(공주, 신라)\n",
            "(부여, 신라)\n",
            "(신라, 공주)\n",
            "(공주, 공주)\n",
            "(부여, 공주)\n",
            "(씨, 공주)\n",
            "(공주, 공주)\n",
            "(부여, 공주)\n",
            "(씨, 공주)\n",
            "(딸, 공주)\n",
            "(공주, 부여)\n",
            "(씨, 부여)\n",
            "(딸, 부여)\n",
            "(며, 부여)\n",
            "(부여, 씨)\n",
            "(딸, 씨)\n",
            "(며, 씨)\n",
            "(였, 씨)\n",
            "(씨, 딸)\n",
            "(며, 딸)\n",
            "(였, 딸)\n",
            "(다, 딸)\n",
            "(딸, 며)\n",
            "(였, 며)\n",
            "(다, 며)\n",
            "(인, 며)\n",
            "(며, 였)\n",
            "(다, 였)\n",
            "(인, 였)\n",
            "(준정, 였)\n",
            "(였, 다)\n",
            "(인, 다)\n",
            "(준정, 다)\n",
            "(신라, 다)\n",
            "(다, 인)\n",
            "(준정, 인)\n",
            "(신라, 인)\n",
            "(초대, 인)\n",
            "(인, 준정)\n",
            "(신라, 준정)\n",
            "(초대, 준정)\n",
            "(여성, 준정)\n",
            "(준정, 신라)\n",
            "(초대, 신라)\n",
            "(여성, 신라)\n",
            "(화랑, 신라)\n",
            "(신라, 초대)\n",
            "(여성, 초대)\n",
            "(화랑, 초대)\n",
            "(였, 초대)\n",
            "(초대, 여성)\n",
            "(화랑, 여성)\n",
            "(였, 여성)\n",
            "(다, 여성)\n",
            "(여성, 화랑)\n",
            "(였, 화랑)\n",
            "(다, 화랑)\n",
            "(준정, 화랑)\n",
            "(화랑, 였)\n",
            "(다, 였)\n",
            "(준정, 였)\n",
            "(화랑, 였)\n",
            "(였, 다)\n",
            "(준정, 다)\n",
            "(화랑, 다)\n",
            "(은, 다)\n",
            "(다, 준정)\n",
            "(화랑, 준정)\n",
            "(은, 준정)\n",
            "(여성, 준정)\n",
            "(준정, 화랑)\n",
            "(은, 화랑)\n",
            "(여성, 화랑)\n",
            "(남성, 화랑)\n",
            "(화랑, 은)\n",
            "(여성, 은)\n",
            "(남성, 은)\n",
            "(게, 은)\n",
            "(은, 여성)\n",
            "(남성, 여성)\n",
            "(게, 여성)\n",
            "(되, 여성)\n",
            "(여성, 남성)\n",
            "(게, 남성)\n",
            "(되, 남성)\n",
            "(었, 남성)\n",
            "(남성, 게)\n",
            "(되, 게)\n",
            "(었, 게)\n",
            "(다, 게)\n"
          ]
        }
      ],
      "source": [
        "# verify (target word, context word)\n",
        "for i, pair in enumerate(dataset):\n",
        "    if i==100:\n",
        "        break\n",
        "    print(f\"({id2word[pair[0]]}, {id2word[pair[1]]})\")\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Z50-Dr4WSn"
      },
      "source": [
        "### 위에서 생성한 `dataset`으로 DataLoader  객체 생성\n",
        "- `DataLoader` 클래스로 `train_dataloader`객체를 생성하라. \n",
        "    - 생성자 매개변수와 값\n",
        "        - dataset = 위에서 생성한 dataset\n",
        "        - batch_size = 64\n",
        "        - shuffle = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:02.645176Z",
          "start_time": "2022-02-19T14:34:02.642780Z"
        },
        "id": "GXcAvFB14WSn"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(dataset=dataset, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:02.777322Z",
          "start_time": "2022-02-19T14:34:02.774335Z"
        },
        "id": "4Yfcwi_14WSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dcbe2ce-d446-4aec-e0b7-b826a015d9fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8301"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTs16gsU4WSn"
      },
      "source": [
        "### Negative Sampling 함수 구현\n",
        "- Skip-Gram은 복잡도를 줄이기 위한 방법으로 negative sampling을 사용한다. \n",
        "- `sample_table`이 다음과 같이 주어졌을 때, sample_table에서 랜덤으로 값을 뽑아 (batch_size, n_neg_sample) shape의 matrix를 반환하는 `get_neg_v_negative_sampling()`함수를 구현하라. \n",
        "- Sample Table은 negative distribution을 따른다. \n",
        "    - [negative distribution 설명](https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#How-are-negative-samples-drawn?)\n",
        "- 함수 정의\n",
        "    - 입력 매개변수\n",
        "        - batch_size : 배치 사이즈, matrix의 row 개수 \n",
        "        - n_neg_sample : negative sample의 개수, matrix의 column 개수\n",
        "    - 반환값 \n",
        "        - neg_v : 추출된 negative sample (2차원의 리스트)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.397509Z",
          "start_time": "2022-02-19T14:34:11.386389Z"
        },
        "id": "PUqIB6dH4WSn",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# negative sample을 추출할 sample table 생성 (해당 코드를 참고)\n",
        "sample_table = []\n",
        "sample_table_size = doc_len\n",
        "\n",
        "# noise distribution 생성\n",
        "alpha = 3/4\n",
        "frequency_list = np.array(list(word2count.values())) ** alpha\n",
        "Z = sum(frequency_list)\n",
        "ratio = frequency_list/Z\n",
        "negative_sample_dist = np.round(ratio*sample_table_size)\n",
        "\n",
        "for wid, c in enumerate(negative_sample_dist):\n",
        "    sample_table.extend([wid]*int(c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.508414Z",
          "start_time": "2022-02-19T14:34:11.505464Z"
        },
        "id": "Wdu8qK8x4WSn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09555795-ed1a-4faf-fe79-53a14d8a8ad4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35179"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "len(sample_table) # word 개수 * sample table size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:11.656046Z",
          "start_time": "2022-02-19T14:34:11.653325Z"
        },
        "id": "mQIVrOIR4WSn"
      },
      "outputs": [],
      "source": [
        "def get_neg_v_negative_sampling(batch_size:int, n_neg_sample:int):\n",
        "    \"\"\"\n",
        "    위에서 정의한 sample_table에서 (batch_size, n_neg_sample) shape만큼 랜덤 추출해 \"네거티브 샘플 메트릭스\"를 생성\n",
        "    np.random.choice() 함수 활용 (위에서 정의한 sample_table을 함수의 argument로 사용)\n",
        "    \"\"\"\n",
        "    neg_v = np.random.choice(sample_table, size= (batch_size,n_neg_sample))\n",
        "    \n",
        "    return neg_v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:12.345976Z",
          "start_time": "2022-02-19T14:34:12.333448Z"
        },
        "id": "8wwT4Af04WSo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d358f930-ce61-4f94-a38b-e72be376b5a7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 918,  778, 1219,  723, 2216],\n",
              "       [2771, 2270, 2364, 2736,  438],\n",
              "       [2213, 1305,  571, 1773, 2497],\n",
              "       [2176, 1305, 1516, 2765, 1398]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "get_neg_v_negative_sampling(4, 5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLnDXPvJ4WSo"
      },
      "source": [
        "## Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5UubCzK4WSo"
      },
      "source": [
        "### 미니 튜토리얼\n",
        "- 아래 튜토리얼을 따라하며 Skip-Gram 모델의 `forward` 및 `loss` 연산 방식을 이해하자\n",
        "- Reference\n",
        "    - [torch.nn.Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
        "    - [torch bmm](https://pytorch.org/docs/stable/generated/torch.bmm.html)\n",
        "    - [Skip-Gram negative sampling loss function 설명 영문 블로그](https://aegis4048.github.io/optimize_computational_efficiency_of_skip-gram_with_negative_sampling#Derivation-of-Cost-Function-in-Negative-Sampling)\n",
        "    - [Skip-Gram negative sampling loss function 설명 한글 블로그](https://reniew.github.io/22/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:46.954048Z",
          "start_time": "2022-02-19T12:51:46.951529Z"
        },
        "id": "IAR68hsY4WSo"
      },
      "outputs": [],
      "source": [
        "# hyper parameter example\n",
        "emb_size = 30000 # vocab size\n",
        "emb_dimension = 300 # word embedding 차원\n",
        "n_neg_sample = 5\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.340056Z",
          "start_time": "2022-02-19T12:51:47.300999Z"
        },
        "id": "zzOsVUn94WSo"
      },
      "outputs": [],
      "source": [
        "# 1. Embedding Matrix와 Context Matrix를 생성\n",
        "u_embedding = nn.Embedding(emb_size, emb_dimension, sparse=True).to(device)\n",
        "v_embedding = nn.Embedding(emb_size, emb_dimension, sparse=True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.352240Z",
          "start_time": "2022-02-19T12:51:49.341437Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7J_ADc44WSo",
        "outputId": "0f5871e9-ddad-4e58-8232-a6494baff432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target word idx : tensor([24460, 10634,  2864, 23952,  3320, 15187, 19625, 26546, 27339,  3920,\n",
            "        25847,  6023,  5055,  7070,  6291, 10245, 15926,   641, 20178,  4565,\n",
            "         4784, 26715, 16955, 28742, 17947, 19774,  8065, 22605,  3061, 28965,\n",
            "         3056, 17963]) Pos context word idx : tensor([23224,  5636, 23712,  5234,  3991, 17897, 25123, 17938, 19634, 24228,\n",
            "          693,   799, 25457,  1308, 28935, 25696,  5601, 23878,  8312,  1292,\n",
            "        21380, 16974,  9318,  9578, 12915, 29271, 26465, 20572,  2362, 25929,\n",
            "        19754, 29080]) Neg context word idx : [[1351 2581 1622 1621 1395]\n",
            " [2314  488 1018 1244 1222]\n",
            " [1680 2146  503 1858  122]\n",
            " [  76 1128 1293  498 1227]\n",
            " [1949  639  486  973 1288]\n",
            " [ 547 2608 2482  334 1187]\n",
            " [1527 1646 1622 2677 2601]\n",
            " [1768 2909 2160 1749 1406]\n",
            " [ 971 1342 1251 2242 1672]\n",
            " [2201 2524 1959 2707  584]\n",
            " [1699 1595  472  285 1212]\n",
            " [1375 1945 1690  674 2436]\n",
            " [1107  153  792 2852 2543]\n",
            " [1668 1347  836  898 2471]\n",
            " [2186 2119 1749 1627 1513]\n",
            " [2910  782  841  404 1581]\n",
            " [1521 1868 2530  421 1200]\n",
            " [1915  294 2438 1891 1066]\n",
            " [1558  917 1523  219 1581]\n",
            " [1966  871 1924 2323 1622]\n",
            " [1971 1664 2658 2237  826]\n",
            " [2973  469 1877 2144 1312]\n",
            " [1730 1244 1955 2197 1886]\n",
            " [1800 1009 2902 2386  495]\n",
            " [1942 1662  457 1650 2595]\n",
            " [2954 1926 2363  921 2768]\n",
            " [ 675  729 2176 1830 2843]\n",
            " [1857  812 1164 1897 2518]\n",
            " [ 723 1281 1014  500 1566]\n",
            " [1918  285 1271 2455  595]\n",
            " [ 395 2948 2059  382 2565]\n",
            " [1214 2413  646 1229  874]]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2. wid(단어 인덱스)를 임의로 생성\n",
        "pos_u = torch.randint(high = emb_size, size = (batch_size,))\n",
        "pos_v = torch.randint(high = emb_size, size = (batch_size,))\n",
        "neg_v = get_neg_v_negative_sampling(batch_size, n_neg_sample)\n",
        "print(f\"Target word idx : {pos_u} Pos context word idx : {pos_v} Neg context word idx : {neg_v}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:49.364020Z",
          "start_time": "2022-02-19T12:51:49.353486Z"
        },
        "id": "4iEG0nCZ4WSo"
      },
      "outputs": [],
      "source": [
        "# 3. tensor로 변환\n",
        "pos_u = Variable(torch.LongTensor(pos_u)).to(device)\n",
        "pos_v = Variable(torch.LongTensor(pos_v)).to(device)\n",
        "neg_v = Variable(torch.LongTensor(neg_v)).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:51.391896Z",
          "start_time": "2022-02-19T12:51:51.387084Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gqbNbajG4WSo",
        "outputId": "63f9422a-f3ef-45d6-f24e-6b9aca5f3eef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of pos_u embedding : torch.Size([32, 300])\n",
            " shape of pos_v embedding : torch.Size([32, 300])\n",
            " shape of neg_v embedding : torch.Size([32, 5, 300])\n"
          ]
        }
      ],
      "source": [
        "# 4. wid로 각각의 embedding matrix에서 word embedding 값을 가져오기\n",
        "pos_u = u_embedding(pos_u)\n",
        "pos_v = v_embedding(pos_v)\n",
        "neg_v = v_embedding(neg_v)\n",
        "print(f\"shape of pos_u embedding : {pos_u.shape}\\n shape of pos_v embedding : {pos_v.shape}\\n shape of neg_v embedding : {neg_v.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:53.121477Z",
          "start_time": "2022-02-19T12:51:52.646148Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDWUrSwo4WSo",
        "outputId": "b3ee24a8-4ab8-403d-eed2-dd3d6df774fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of pos logits : torch.Size([32])\n",
            "\n",
            "shape of logits : torch.Size([32, 5])\n"
          ]
        }
      ],
      "source": [
        "# 5. dot product \n",
        "pos_score = torch.mul(pos_u, pos_v) # 행렬 element-wise 곱\n",
        "pos_score = torch.sum(pos_score, dim=1)\n",
        "print(f\"shape of pos logits : {pos_score.shape}\\n\")\n",
        "\n",
        "# input : (b,5,300) , (b,300, -1) -> (b,5) (squeezed) \n",
        "neg_score = torch.bmm(neg_v, pos_u.unsqueeze(dim=2)).squeeze() # batch 별 matrix 연산\n",
        "print(f\"shape of logits : {neg_score.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T12:51:53.670418Z",
          "start_time": "2022-02-19T12:51:53.665671Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "adOpcoL54WSo",
        "outputId": "852d8299-f18c-4e96-cdde-6a659e046380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-7.4414e+00, -1.5843e-01, -5.9454e+00, -2.3842e-07, -7.5337e-05,\n",
            "        -1.1106e+01, -0.0000e+00, -2.7479e+01, -1.8868e+01, -9.7658e-02,\n",
            "        -1.4674e+01, -0.0000e+00, -1.0167e+01, -1.5579e-04, -1.3113e-06,\n",
            "        -0.0000e+00, -1.3921e+01, -1.9878e-02, -7.5666e+00, -3.9380e+01,\n",
            "        -7.6009e+00, -2.7195e+00, -3.5763e-07, -5.8717e-04, -2.3842e-07,\n",
            "        -6.5446e+00, -1.1921e-07, -3.1243e+01, -1.1446e+00, -2.3842e-07,\n",
            "        -9.0385e+00, -2.6304e+01], device='cuda:0',\n",
            "       grad_fn=<LogSigmoidBackward0>)\n",
            "tensor([[-9.5824e+00, -4.5648e-02, -1.3385e-02, -0.0000e+00, -2.9583e-04],\n",
            "        [-2.0766e-03, -8.4657e+00, -1.7236e-04, -9.8981e+00, -8.0168e+00],\n",
            "        [-2.0449e+01, -4.2255e-01, -8.7510e-03, -8.3311e-03, -1.1063e+01],\n",
            "        [-6.1989e-06, -1.8432e+01, -1.9700e+01, -0.0000e+00, -5.4819e+00],\n",
            "        [-1.1330e+01, -3.9349e-03, -1.9016e+01, -2.3590e-03, -0.0000e+00],\n",
            "        [-1.0307e-02, -0.0000e+00, -1.7200e-01, -1.2480e-04, -7.9647e+00],\n",
            "        [-3.5763e-07, -2.1668e-02, -5.6554e-03, -1.3990e+01, -1.0729e-06],\n",
            "        [-1.1921e-07, -1.4590e-04, -9.4657e+00, -9.5391e+00, -1.6212e-05],\n",
            "        [-1.1119e+01, -0.0000e+00, -3.0504e-01, -1.4131e+01, -1.1395e-02],\n",
            "        [-3.5763e-07, -1.6891e-01, -2.0696e+01, -2.1823e+01, -0.0000e+00],\n",
            "        [-1.5332e+01, -6.0403e+00, -1.7762e-05, -2.3842e-07, -2.3634e+01],\n",
            "        [-8.1883e+00, -5.5181e-03, -1.1164e+01, -7.0333e-06, -1.4970e+00],\n",
            "        [-0.0000e+00, -6.4763e+00, -3.2238e+00, -5.6537e-04, -0.0000e+00],\n",
            "        [-2.4154e+00, -1.1881e+01, -0.0000e+00, -1.2383e+01, -0.0000e+00],\n",
            "        [-2.3842e-07, -6.1510e-05, -0.0000e+00, -3.0352e-02, -4.2720e-01],\n",
            "        [-5.0957e-01, -0.0000e+00, -7.5003e+00, -1.1727e+01, -2.5944e+01],\n",
            "        [-1.0292e-03, -3.0734e-01, -1.2013e-03, -1.1979e+01, -0.0000e+00],\n",
            "        [-2.0654e+00, -2.0550e+01, -2.9399e-01, -3.1352e-05, -2.5968e+00],\n",
            "        [-5.2028e+00, -3.7610e-02, -2.9197e+01, -3.3174e+00, -3.5763e-07],\n",
            "        [-0.0000e+00, -2.0950e+01, -1.7851e+01, -1.5497e-05, -2.3586e+01],\n",
            "        [-1.4228e+01, -0.0000e+00, -2.1558e+00, -4.5795e+00, -1.9800e-03],\n",
            "        [-4.8515e-02, -2.3842e-07, -2.0027e-05, -0.0000e+00, -0.0000e+00],\n",
            "        [-0.0000e+00, -2.4712e+01, -1.1233e-02, -1.5182e+01, -8.7326e+00],\n",
            "        [-0.0000e+00, -3.6224e+01, -0.0000e+00, -0.0000e+00, -1.2406e+01],\n",
            "        [-6.1412e+00, -3.9659e+01, -5.7701e+00, -3.9951e-04, -1.0458e-01],\n",
            "        [-6.1367e-02, -3.3965e+01, -7.8690e-03, -2.5758e-04, -7.7486e-06],\n",
            "        [-7.6564e+00, -2.6033e-02, -2.0168e-04, -5.4550e+00, -1.4241e+01],\n",
            "        [-3.5763e-07, -1.3784e+00, -0.0000e+00, -9.6773e+00, -1.7992e+01],\n",
            "        [-1.4740e+00, -1.2394e+01, -8.0906e-02, -4.0118e-04, -3.7607e+00],\n",
            "        [-4.7628e+00, -4.5731e+00, -1.2971e+01, -3.9162e+01, -0.0000e+00],\n",
            "        [-5.2452e-06, -9.3727e+00, -0.0000e+00, -0.0000e+00, -1.4995e-04],\n",
            "        [-1.2393e+01, -2.1176e+01, -7.8138e+00, -5.2202e-01, -4.7684e-07]],\n",
            "       device='cuda:0', grad_fn=<LogSigmoidBackward0>)\n",
            "pos logits : -241.4199676513672\n",
            "neg logits : -928.5562744140625\n",
            "Loss : 1169.9761962890625\n"
          ]
        }
      ],
      "source": [
        "# 6. loss 구하기\n",
        "pos_score = F.logsigmoid(pos_score)\n",
        "print(pos_score)\n",
        "neg_score = F.logsigmoid(-1*neg_score) # negative의 logit은 minimize 하기 위해 -1 곱함\n",
        "print(neg_score)\n",
        "print(f\"pos logits : {pos_score.sum()}\")\n",
        "print(f\"neg logits : {neg_score.sum()}\")\n",
        "loss = -1 * (torch.sum(pos_score) + torch.sum(neg_score))\n",
        "print(f\"Loss : {loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muEceOGZ4WSo"
      },
      "source": [
        "### Skip-gram 클래스 구현\n",
        "- Skip-Gram 방식으로 단어 embedding을 학습하는 `SkipGram` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()` 함수) 입력 매개변수\n",
        "        - `vocab_size` : 사전내 단어 개수\n",
        "        - `emb_dimension` : 엠베딩 크기\n",
        "        - `device` : 연산 장치 종류\n",
        "    - 생성자에서 생성해야할 변수 \n",
        "        - `vocab_size` : 사전내 단어 개수\n",
        "        - `emb_dimension` : 엠베딩 크기\n",
        "        - `u_embedding` : (vocab_size, emb_dimension) 엠베딩 메트릭스 (target_word)\n",
        "        - `v_embedding` : (vocab_size, emb_dimension) 엠베딩 메트릭스 (context_word)\n",
        "    - 메소드\n",
        "        - `init_embedding()` (제공됨)\n",
        "            - 엠베딩 메트릭스 값을 초기화\n",
        "        - `forward()`\n",
        "            - 위 튜토리얼과 같이 dot product를 수행한 후 score를 생성\n",
        "            - loss를 반환 (loss 설명 추가)\n",
        "        - `save_emedding()` (제공됨)\n",
        "            - `u_embedding`의 단어 엠베딩 값을 단어 별로 파일에 저장\n",
        "    - 주의 사항     \n",
        "        - `nn.Module`를 부모 클래스로 상속 받음 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:15.731306Z",
          "start_time": "2022-02-19T14:34:15.721129Z"
        },
        "id": "pnmMamP44WSo"
      },
      "outputs": [],
      "source": [
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size:int, emb_dimension:int, device:str):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.u_embedding = nn.Embedding(vocab_size, emb_dimension, sparse=True).to(device)\n",
        "        self.v_embedding = nn.Embedding(vocab_size, emb_dimension, sparse=True).to(device)\n",
        "        self.init_embedding()\n",
        "    \n",
        "    \n",
        "    def init_embedding(self):\n",
        "        \"\"\"\n",
        "        u_embedding과 v_embedding 메트릭스 값을 초기화\n",
        "        \"\"\"\n",
        "        initrange = 0.5 / self.emb_dimension\n",
        "        self.u_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.v_embedding.weight.data.uniform_(-0, 0) # u, v matrix를 initialize\n",
        "    \n",
        "    \n",
        "    def forward(self, pos_u, pos_v, neg_v):\n",
        "        \"\"\"\n",
        "        dot product를 수행한 후 score를 생성\n",
        "        loss 반환\n",
        "        \"\"\"    \n",
        "            \n",
        "        # 각각의 embedding matrix에서 word embedding 값을 가져오기\n",
        "        pos_u = self.u_embedding(pos_u)\n",
        "        pos_v = self.v_embedding(pos_v)\n",
        "        neg_v = self.v_embedding(neg_v)\n",
        "        \n",
        "        # dot product \n",
        "        pos_score = torch.sum(torch.mul(pos_u, pos_v),dim=1) #torch.mul 사용 후 vocab 별로 합 구하기\n",
        "        # input : (b,n_neg_sample,dimension) , (b,dimension, -1) -> (b,5) (squeezed) \n",
        "        neg_score = torch.bmm(neg_v, pos_u.unsqueeze(dim=2)).squeeze() # batch 별 matrix 연산\n",
        "        \n",
        "        # loss 구하기\n",
        "        pos_score = F.logsigmoid(pos_score)\n",
        "        neg_score = F.logsigmoid(-1*neg_score) # negative의 logit은 minimize 하기 위해 -1 곱함\n",
        "\n",
        "        loss = -1 * (torch.sum(pos_score) + torch.sum(neg_score))\n",
        "\n",
        "        return loss\n",
        "    \n",
        "    def save_embedding(self, id2word, file_name, use_cuda):\n",
        "        \"\"\"\n",
        "        'file_name' 위치에 word와 word_embedding을 line-by로 저장\n",
        "        파일의 첫 줄은 '단어 개수' 그리고 '단어 embedding 사이즈' 값을 입력해야 함\n",
        "        \"\"\"\n",
        "        if use_cuda: # parameter를 gpu 메모리에서 cpu 메모리로 옮김\n",
        "            embedding = self.u_embedding.weight.cpu().data.numpy()\n",
        "        else:\n",
        "            embedding = self.u_embedding.weight.data.numpy()\n",
        "\n",
        "        with open(file_name, 'w') as writer:\n",
        "            # 파일의 첫 줄은 '단어 개수' 그리고 '단어 embedding 사이즈' 값을 입력해야 함\n",
        "            writer.write(f\"{len(id2word)} {embedding.shape[-1]}\\n\")\n",
        "            \n",
        "            for wid, word in id2word.items():\n",
        "                e = embedding[wid]\n",
        "                e = \" \".join([str(e_) for e_ in e])\n",
        "                writer.write(f\"{word} {e}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqqMo0zL4WSo"
      },
      "source": [
        "## Advanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSWd5gV24WSp"
      },
      "source": [
        "### Skip-Gram 방식의  Word2Vec 클래스 구현\n",
        "- Skip-Gram 방식으로 단어 embedding을 학습하는 `Word2Vec` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "    - 생성자(`__init__()`) 입력 매개 변수\n",
        "        - `input_file` : 학습할 문서 리스트\n",
        "        - `output_file_name` : 학습된 word embedding을 저장할 파일 위치\n",
        "        - `device` : 연상 장치 종류\n",
        "        - `emb_dimension` : word embedding 차원\n",
        "        - `batch_size` : 학습 배치 사이즈\n",
        "        - `window_size` : skip-gram 윈도우 사이즈 (context word 개수를 결정)\n",
        "        - `n_neg_sample` : negative sample 개수\n",
        "        - `iteration` : 학습 반복 횟수\n",
        "        - `lr` : learning rate\n",
        "        - `min_count` : 사전에 추가될 단어의 최소 등장 빈도\n",
        "    - 생성자에서 생성해야 할 변수 \n",
        "        - `docs` : 학습할 문서 리스트\n",
        "        - `output_file_name` : 학습된 word embedding을 저장할 파일 위치\n",
        "        - `word2count`, `word2id`, `id2word` : 위에서 구현한 `make_vocab()` 함수의 반환 값\n",
        "        - `device` : 연산 장치 종류\n",
        "        - `emb_size` : vocab의 (unique한) 단어 종류 \n",
        "        - `emb_dimension` : word embedding 차원\n",
        "        - `batch_size` : 학습 배치 사이즈\n",
        "        - `window_size` : skip-gram 윈도우 사이즈 (context word 개수를 결정)\n",
        "        - `n_neg_sample` : negative sample 개수\n",
        "        - `iteration` : 학습 반복 횟수\n",
        "        - `lr` : learning rate\n",
        "        - `model` : `SkipGram` 클래스의 인스턴스\n",
        "        - `optimizer` : `SGD` 클래스의 인스턴스\n",
        "    - 메소드\n",
        "        - `train()`\n",
        "            - 입력 매개변수 \n",
        "                - `train_dataloader`\n",
        "            - Iteration 횟수만큼 input_file 학습 데이터를 학습한다. 매 epoch마다 for loop 돌면서 batch 단위 학습 데이터를 skip gram 모델에 학습함. 학습이 끝나면 word embedding을 output_file_name 파일에 저장.\n",
        "- Reference\n",
        "    - [Optimizer - SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:20.503555Z",
          "start_time": "2022-02-19T14:34:20.491585Z"
        },
        "id": "Td-GQrqI4WSp"
      },
      "outputs": [],
      "source": [
        "from torch.nn.modules.loss import L1Loss\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "class Word2Vec:\n",
        "    def __init__(self, \n",
        "                input_file: List[str],\n",
        "                output_file_name: str,\n",
        "                 device: str,\n",
        "                 emb_dimension=300,\n",
        "                 batch_size = 64,\n",
        "                 window_size=5,\n",
        "                 n_neg_sample = 5,\n",
        "                 iteration=1,\n",
        "                 lr = 0.02,\n",
        "                 min_count=5):\n",
        "        self.docs = input_file\n",
        "        self.output_file_name = '/content/output/'\n",
        "        self.word2count, self.word2id, self.id2word = make_vocab(self.docs, min_count)\n",
        "        self.device = device\n",
        "        self.emb_size = len(self.word2id)+1\n",
        "        self.emb_dimension = emb_dimension\n",
        "        self.batch_size = batch_size\n",
        "        self.window_size = window_size\n",
        "        self.n_neg_sample = n_neg_sample\n",
        "        self.iteration = iteration\n",
        "        self.lr = lr\n",
        "        # (self, vocab_size:int, emb_dimension:int, device:str)\n",
        "        self.model = SkipGram(self.emb_size, self.emb_dimension, self.device)\n",
        "        self.optimizer = torch.optim.SGD(self.model.parameters(),lr=self.lr) # torch.optim.SGD 클래스 사용\n",
        "        # self.loss_fn=L1Loss()\n",
        "\n",
        "        # train() 함수에서 만든 임베딩 결과 파일들을 저장할 폴더 생성 (os.makedirs 사용)\n",
        "        if not os.path.exists('/content/output'):\n",
        "          os.makedirs('/content/output')\n",
        "        \n",
        "    \n",
        "    def train(self, train_dataloader):\n",
        "        \n",
        "        # lr 값을 조절하는 스케줄러 인스턴스 변수를 생성\n",
        "        self.scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer = self.optimizer,\n",
        "            num_warmup_steps=0,\n",
        "            num_training_steps=self.iteration*len(train_dataloader)\n",
        "        )\n",
        "        \n",
        "        for epoch in range(self.iteration):\n",
        "            \n",
        "            print(f\"*****Epoch {epoch} Train Start*****\")\n",
        "            print(f\"*****Epoch {epoch} Total Step {len(train_dataloader)}*****\")\n",
        "            total_loss, batch_loss, batch_step = 0,0,0\n",
        "\n",
        "            for step, batch in enumerate(train_dataloader):\n",
        "                batch_step+=1\n",
        "\n",
        "                pos_u, pos_v = batch\n",
        "                # negative data 생성\n",
        "                neg_v = get_neg_v_negative_sampling(pos_u.shape[0], self.n_neg_sample)\n",
        "                \n",
        "                # 데이터를 tensor화 & device 설정\n",
        "                pos_u = Variable(torch.LongTensor(pos_u)).to(device)\n",
        "                pos_v = Variable(torch.LongTensor(pos_v)).to(device)\n",
        "                neg_v = Variable(torch.LongTensor(neg_v)).to(device)\n",
        "\n",
        "                # model의 gradient 초기화\n",
        "                self.model.zero_grad()\n",
        "                # optimizer의 gradient 초기화\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                loss = self.model.forward(pos_u, pos_v, neg_v)\n",
        "                \n",
        "                \n",
        "\n",
        "                \n",
        "\n",
        "                batch_loss += loss.item()\n",
        "                total_loss += loss.item()\n",
        "                # loss 계산\n",
        "                loss.backward()\n",
        "                # print(loss.item())\n",
        "            \n",
        "                # optimizer 업데이트\n",
        "                self.optimizer.step()\n",
        "                # scheduler 업데이트\n",
        "                self.scheduler.step()\n",
        "\n",
        "                \n",
        "                \n",
        "                \n",
        "                if (step%500 == 0) and (step!=0):\n",
        "                    print(f\"Step: {step} Loss: {batch_loss/batch_step:.4f} lr: {self.optimizer.param_groups[0]['lr']:.4f}\")\n",
        "                    # 변수 초기화    \n",
        "                    batch_loss, batch_step = 0,0\n",
        "            \n",
        "            print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "            print(f\"*****Epoch {epoch} Train Finished*****\\n\")\n",
        "            \n",
        "            print(f\"*****Epoch {epoch} Saving Embedding...*****\")\n",
        "            self.model.save_embedding(self.id2word, os.path.join(self.output_file_name, f'w2v_{epoch}.txt'), True if 'cuda' in self.device.type else False)\n",
        "            print(f\"*****Epoch {epoch} Embedding Saved at {os.path.join(self.output_file_name, f'w2v_{epoch}.txt')}*****\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:29.561892Z",
          "start_time": "2022-02-19T14:34:26.103659Z"
        },
        "id": "Ywx9R8n24WSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac459cf0-0b9c-4954-de82-0268be7bf057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 500/500 [01:25<00:00,  5.86it/s]\n"
          ]
        }
      ],
      "source": [
        "output_file = os.path.join(\".\", \"word2vec_wiki\")\n",
        "# Word2Vec 클래스의 인스턴스 생성\n",
        "w2v = Word2Vec(docs, output_file, device, n_neg_sample=10, iteration=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:34:34.615469Z",
          "start_time": "2022-02-19T14:34:34.055502Z"
        },
        "id": "ufBxjKxN4WSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60ffe9ca-ef66-4083-cb73-e0c2e4ec555e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8301"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "# 학습 데이터 셋 및 데이터 로더 생성 (위에서 생성한 w2v의 attribute들을 argument에 적절히 넣기)\n",
        "\n",
        "# CustomDataset(docs, word2id, window_size=5)\n",
        "dataset = CustomDataset(w2v.docs, w2v.word2id, w2v.window_size)\n",
        "\n",
        "# train_dataloader = DataLoader(dataset=dataset, batch_size, shuffle=True)\n",
        "train_dataloader = DataLoader(dataset=dataset, batch_size=w2v.batch_size, shuffle=True)\n",
        "len(train_dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:45:38.362817Z",
          "start_time": "2022-02-19T14:34:37.382371Z"
        },
        "id": "9JBUrUJ34WSp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f798671-b580-470c-b679-c94c8136a0b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****Epoch 0 Train Start*****\n",
            "*****Epoch 0 Total Step 8301*****\n",
            "Step: 500 Loss: 486.7337 lr: 0.0196\n",
            "Step: 1000 Loss: 382.7275 lr: 0.0192\n",
            "Step: 1500 Loss: 264.1092 lr: 0.0188\n",
            "Step: 2000 Loss: 211.8946 lr: 0.0184\n",
            "Step: 2500 Loss: 184.8337 lr: 0.0180\n",
            "Step: 3000 Loss: 171.0326 lr: 0.0176\n",
            "Step: 3500 Loss: 164.2032 lr: 0.0172\n",
            "Step: 4000 Loss: 159.1978 lr: 0.0168\n",
            "Step: 4500 Loss: 155.2258 lr: 0.0164\n",
            "Step: 5000 Loss: 153.7880 lr: 0.0160\n",
            "Step: 5500 Loss: 151.7665 lr: 0.0156\n",
            "Step: 6000 Loss: 150.1942 lr: 0.0152\n",
            "Step: 6500 Loss: 149.1978 lr: 0.0148\n",
            "Step: 7000 Loss: 148.1623 lr: 0.0144\n",
            "Step: 7500 Loss: 146.4712 lr: 0.0140\n",
            "Step: 8000 Loss: 146.5109 lr: 0.0136\n",
            "Epoch 0 Total Mean Loss : 199.6032\n",
            "*****Epoch 0 Train Finished*****\n",
            "\n",
            "*****Epoch 0 Saving Embedding...*****\n",
            "*****Epoch 0 Embedding Saved at /content/output/w2v_0.txt*****\n",
            "\n",
            "*****Epoch 1 Train Start*****\n",
            "*****Epoch 1 Total Step 8301*****\n",
            "Step: 500 Loss: 145.0469 lr: 0.0129\n",
            "Step: 1000 Loss: 143.7887 lr: 0.0125\n",
            "Step: 1500 Loss: 143.5078 lr: 0.0121\n",
            "Step: 2000 Loss: 142.6892 lr: 0.0117\n",
            "Step: 2500 Loss: 141.9585 lr: 0.0113\n",
            "Step: 3000 Loss: 141.1555 lr: 0.0109\n",
            "Step: 3500 Loss: 141.7949 lr: 0.0105\n",
            "Step: 4000 Loss: 140.7237 lr: 0.0101\n",
            "Step: 4500 Loss: 140.6341 lr: 0.0097\n",
            "Step: 5000 Loss: 139.7055 lr: 0.0093\n",
            "Step: 5500 Loss: 139.6708 lr: 0.0089\n",
            "Step: 6000 Loss: 138.7766 lr: 0.0085\n",
            "Step: 6500 Loss: 138.9179 lr: 0.0081\n",
            "Step: 7000 Loss: 139.1904 lr: 0.0077\n",
            "Step: 7500 Loss: 137.7736 lr: 0.0073\n",
            "Step: 8000 Loss: 138.4036 lr: 0.0069\n",
            "Epoch 1 Total Mean Loss : 140.7169\n",
            "*****Epoch 1 Train Finished*****\n",
            "\n",
            "*****Epoch 1 Saving Embedding...*****\n",
            "*****Epoch 1 Embedding Saved at /content/output/w2v_1.txt*****\n",
            "\n",
            "*****Epoch 2 Train Start*****\n",
            "*****Epoch 2 Total Step 8301*****\n",
            "Step: 500 Loss: 136.5670 lr: 0.0063\n",
            "Step: 1000 Loss: 137.4194 lr: 0.0059\n",
            "Step: 1500 Loss: 136.4912 lr: 0.0055\n",
            "Step: 2000 Loss: 136.2358 lr: 0.0051\n",
            "Step: 2500 Loss: 136.5921 lr: 0.0047\n",
            "Step: 3000 Loss: 135.7879 lr: 0.0043\n",
            "Step: 3500 Loss: 135.3838 lr: 0.0039\n",
            "Step: 4000 Loss: 136.4976 lr: 0.0035\n",
            "Step: 4500 Loss: 136.3157 lr: 0.0031\n",
            "Step: 5000 Loss: 134.8546 lr: 0.0027\n",
            "Step: 5500 Loss: 136.2228 lr: 0.0022\n",
            "Step: 6000 Loss: 135.4612 lr: 0.0018\n",
            "Step: 6500 Loss: 135.0731 lr: 0.0014\n",
            "Step: 7000 Loss: 135.9204 lr: 0.0010\n",
            "Step: 7500 Loss: 135.3781 lr: 0.0006\n",
            "Step: 8000 Loss: 135.7033 lr: 0.0002\n",
            "Epoch 2 Total Mean Loss : 135.9432\n",
            "*****Epoch 2 Train Finished*****\n",
            "\n",
            "*****Epoch 2 Saving Embedding...*****\n",
            "*****Epoch 2 Embedding Saved at /content/output/w2v_2.txt*****\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 학습\n",
        "\n",
        "w2v.train(train_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uTIm4vJ4WSp"
      },
      "source": [
        "### 유사한 단어 확인\n",
        "- 사전에 존재하는 단어들과 유사한 단어를 검색해보자. Gensim 패키지는 유사 단어 외에도 단어간의 유사도를 계산하는 여러 함수를 제공한다. 실험을 통해 word2vec의 한계점을 발견했다면 아래에 markdown으로 작성해보자. \n",
        "- [Gensim 패키지 document](https://radimrehurek.com/gensim/models/keyedvectors.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:47:59.605389Z",
          "start_time": "2022-02-19T14:47:59.368925Z"
        },
        "id": "AKpBuVlP4WSp"
      },
      "outputs": [],
      "source": [
        "import gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:49:06.590460Z",
          "start_time": "2022-02-19T14:49:05.174241Z"
        },
        "id": "AWTCodimsAq8"
      },
      "outputs": [],
      "source": [
        "word_vectors = gensim.models.KeyedVectors.load_word2vec_format('/content/output/w2v_0.txt', binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2022-02-19T14:49:11.324372Z",
          "start_time": "2022-02-19T14:49:11.315429Z"
        },
        "id": "MLMh_evrsAq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e105f876-b20f-41da-a408-81c7507bfd8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('구체', 0.9998613595962524),\n",
              " ('동물', 0.9998601675033569),\n",
              " ('집단', 0.9998598694801331),\n",
              " ('다루', 0.9998542666435242),\n",
              " ('국민주의', 0.9998542070388794),\n",
              " ('실험', 0.9998539686203003),\n",
              " ('바', 0.999853253364563),\n",
              " ('우주', 0.9998530745506287),\n",
              " ('교황', 0.99985271692276),\n",
              " ('차지', 0.9998518824577332)]"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "word_vectors.most_similar(positive='삼겹살')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "word2vec의 한계점은?\n",
        "- 실제로 매우 유사한 단어로 출력되는 것들이 정성적으로 납득이 잘 되지 않았다. 한 마디로, Negative sampling을 하고 skip-gram방식으로 학습하는 것이 생각보다 효과적이지는 않은 것 같다. training dataset이 작아서 그런 것도 있겠지만, 한마디로 성능이 좋지 않다. \n",
        "- 형태소(morpheme) 또는 subword 별로 의미론을 판단하기가 힘든 것들이 있다. 위의 결과 중 '국민' 과 '주의' 같은 경우 각기 다른 의미의 단어로 분해가 가능하다. Fasttext 모델로 개선이 가능한 것으로 알고 있는데 별도로 실험을 해보면 좋을 것 같다."
      ],
      "metadata": {
        "id": "X8lc8NQe4cT2"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Jesung Ryu - Week3_1_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}