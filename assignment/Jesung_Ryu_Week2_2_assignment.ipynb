{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592U6lXs3d2t"
      },
      "source": [
        "# Week2_2 Assignment\n",
        "\n",
        "## [BASIC](#Basic) \n",
        "- \"네이버 영화 감성 분류\" 데이터를 불러와 `pandas` 라이브러리를 사용해 **전처리** 할 수 있다.\n",
        "- 적은 데이터로도 높은 성능을 내기 위해, pre-trained `BERT` 모델 위에 1개의 hidden layer를 쌓아 **fine-tuning**할 수 있다.\n",
        "\n",
        "## [CHALLENGE](#Challenge)\n",
        "- 토큰화된 학습 데이터를 배치 단위로 갖는 **traindata iterator**를 구현할 수 있다. \n",
        "\n",
        "## [ADVANCED](#Advanced)\n",
        "- **loss와 optimizer 함수**를 사용할 수 있다. \n",
        "- traindata iterator를 for loop 돌며 **fine-tuning** 할 수 있다.\n",
        "- fine-tuning의 2가지 방법론을 비교할 수 있다. \n",
        "  - BERT 파라미터를 **freeze** 한 채 fine-tuning (Vision에서 주로 사용하는 방법론)\n",
        "  - BERT 파라미터를 **unfreeze** 한 채 fine-tuning (NLP에서 주로 사용하는 방법론)\n",
        "\n",
        "\n",
        "### Reference\n",
        "- [huggingface 한국어 오픈소스 모델](https://huggingface.co/models?language=ko&sort=downloads&search=bert)\n",
        "- [transformer BertForSequenceClassification 소스 코드](https://github.com/huggingface/transformers/blob/v4.15.0/src/transformers/models/bert/modeling_bert.py#L1501)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSX-wQA1RD1h"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import torch\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Reyt-HvLnJv"
      },
      "outputs": [],
      "source": [
        "# seed\n",
        "seed = 7777\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gUR6vb3L3d2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d3a77bf-0199-412a-d79c-10059a5c8b92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla P100-PCIE-16GB\n",
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# device type\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "  print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c93M8XmjLnJw"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0REKl4EvT9G1"
      },
      "source": [
        "### 데이터 다운로드 및 DataFrame 형태로 불러오기\n",
        "- 내 구글 드라이브에 데이터를 다운받은 후 코랩에 드라이브를 마운트하면 데이터를 영구적으로 사용할 수 있음.\n",
        "- [네이버영화감성분류](https://github.com/e9t/nsmc)\n",
        "  - trainset: 150,000 \n",
        "  - testset: 50,000 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEWUggR1R9rS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "439e31e3-0a9e-47e6-80bf-a494d4685e79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rov1s8IxSLqy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a07c80ec-c80e-4baf-b85d-0a9f2655ce45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/[Wanted]week2-2\n"
          ]
        }
      ],
      "source": [
        "cd \"/content/drive/MyDrive/[Wanted]week2-2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjPGnbEjVYmj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "172330dd-7dc5-4261-a901-d4f22a66750d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'nsmc' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# 데이터 다운로드\n",
        "!git clone https://github.com/e9t/nsmc.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SueG9v14YbgF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39dbbebb-de95-4ee1-a868-9456a226942c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My current directory : /content/drive/MyDrive/[Wanted]week2-2\n"
          ]
        }
      ],
      "source": [
        "_CUR_DIR = os.path.abspath(os.curdir)\n",
        "print(f\"My current directory : {_CUR_DIR}\")\n",
        "_DATA_DIR = os.path.join(_CUR_DIR, \"nsmc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9J6KQ8dzaHBi"
      },
      "outputs": [],
      "source": [
        "# nsmc/ratings_train.txt를 DataFrame 형태로 불러오기\n",
        "df = pd.read_csv(os.path.join(_DATA_DIR, \"ratings_train.txt\"), sep='\\t') #tsv 파일이므로 tab으로 구분"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> tab으로 구분되어진 "
      ],
      "metadata": {
        "id": "Hnic8FpRn9Gb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3cUsoBEPahlo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4de2330-8de8-423f-c9ea-b4e234fa983e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(150000, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "# 데이터 크기 확인\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ic3k9CORaXzM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "0c2438d7-9727-4e6d-decd-233c20b571e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-fdb59624-284a-4b39-aa51-8a1c7129041b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fdb59624-284a-4b39-aa51-8a1c7129041b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fdb59624-284a-4b39-aa51-8a1c7129041b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fdb59624-284a-4b39-aa51-8a1c7129041b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         id                                           document  label\n",
              "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2  10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ],
      "source": [
        "# 데이터 일부 확인\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA1F0tHWLnJz"
      },
      "source": [
        "### 데이터 결측치 제거 및 데이터 수 줄이기 \n",
        "- 학습 데이터 수는 150,000개로 매우 많은 양이다. 하지만 우리가 실생활에서 마주할 데이터는 이렇게 많지 않다. 이 때 유용하게 사용되는 것이 **fine-tuning** 학습 방법이다.   \n",
        "- Fine-tuning은 단어의 의미를 이미 충분히 학습한 모델 (여기서는 **BERT**)을 가져와 그 위에 추가적인 Nueral Network 레이어를 쌓은 후 학습하는 방법론이다. 이미 BERT가 단어의 의미를 충분히 학습했기 때문에 **적은 데이터**로 학습해도 우수한 성능을 낼 수 있다는 장점이 있다. \n",
        "- **데이터의 label의 비율이 5:5를 유지하면서** 학습 데이터 수를 150,000개에서 1,000개로 줄이\b는 함수 `label_evenly_balanced_dataset_sampler`를 구현하라.\n",
        "  - 함수 정의 \n",
        "    - 입력 매개변수\n",
        "      - df : DataFrame\n",
        "      - n_sample : df에서 샘플링할 row의 개수 (여기서는 1000개로 정의한다)\n",
        "    - 조건\n",
        "      - label의 비율이 5:5를 유지할 수 있도록 샘플링한다.\n",
        "    - 반환값\n",
        "      - row의 개수가 1000개인 dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lh9BSiSeMms7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "outputId": "26533b27-f60d-4b1b-9674-db1d990fe59a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-2a98d3e3-c3f6-4b1e-b1d0-5cb6cff97ac7\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>document</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9976970</td>\n",
              "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3819312</td>\n",
              "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10265843</td>\n",
              "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>9045019</td>\n",
              "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6483659</td>\n",
              "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149995</th>\n",
              "      <td>6222902</td>\n",
              "      <td>인간이 문제지.. 소는 뭔죄인가..</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149996</th>\n",
              "      <td>8549745</td>\n",
              "      <td>평점이 너무 낮아서...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149997</th>\n",
              "      <td>9311800</td>\n",
              "      <td>이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149998</th>\n",
              "      <td>2376369</td>\n",
              "      <td>청춘 영화의 최고봉.방황과 우울했던 날들의 자화상</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149999</th>\n",
              "      <td>9619869</td>\n",
              "      <td>한국 영화 최초로 수간하는 내용이 담긴 영화</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>149995 rows × 3 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a98d3e3-c3f6-4b1e-b1d0-5cb6cff97ac7')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2a98d3e3-c3f6-4b1e-b1d0-5cb6cff97ac7 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2a98d3e3-c3f6-4b1e-b1d0-5cb6cff97ac7');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "              id                                           document  label\n",
              "0        9976970                                아 더빙.. 진짜 짜증나네요 목소리      0\n",
              "1        3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1\n",
              "2       10265843                                  너무재밓었다그래서보는것을추천한다      0\n",
              "3        9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0\n",
              "4        6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1\n",
              "...          ...                                                ...    ...\n",
              "149995   6222902                                인간이 문제지.. 소는 뭔죄인가..      0\n",
              "149996   8549745                                      평점이 너무 낮아서...      1\n",
              "149997   9311800                    이게 뭐요? 한국인은 거들먹거리고 필리핀 혼혈은 착하다?      0\n",
              "149998   2376369                        청춘 영화의 최고봉.방황과 우울했던 날들의 자화상      1\n",
              "149999   9619869                           한국 영화 최초로 수간하는 내용이 담긴 영화      0\n",
              "\n",
              "[149995 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ],
      "source": [
        "# df에서 결측치 (na 값) 제거\n",
        "\n",
        "df = df.dropna(axis=0)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yoUEDy6TQJj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ommF5KH4akCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9cb1e6a-b4ec-47d1-d7c5-4c2eb5600b84"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    75170\n",
              "1    74825\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ],
      "source": [
        "# label별 데이터 수 확인\n",
        "# pandas의 value_counts 함수 활용\n",
        "# 0 -> 부정 1 -> 긍정\n",
        "df.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ii06wCsc107"
      },
      "outputs": [],
      "source": [
        "# 학습 데이터 샘플 개수 설정\n",
        "\n",
        "n_sample = 1000"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df[df['label']==0]"
      ],
      "metadata": {
        "id": "47DzFdtnzu_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hrkhl69Dc-kr"
      },
      "outputs": [],
      "source": [
        "# 샘플링 함수 구현\n",
        "# random 모듈에서 제공되는 함수 활용\n",
        "# input: 학습 데이터 샘플 개수\n",
        "# output: 샘플링 데이터\n",
        "\n",
        "\n",
        "def label_evenly_balanced_dataset_sampler(df, sample_size):\n",
        "  \"\"\"\n",
        "  데이터 프레임의을 sample_size만큼 임의 추출해 새로운 데이터 프레임을 생성.\n",
        "  이 때, \"label\"열의 값들이 동일한 비율을 갖도록(5:5) 할 것.\n",
        "  \"\"\"\n",
        "  split_size=sample_size//2\n",
        "  if sample_size%2==0: #짝수면 반으로 나눔\n",
        "    \n",
        "    df1 = df[df['label']==1].sample(n=split_size)\n",
        "    df2 = df[df['label']==0].sample(n=split_size)\n",
        "  \n",
        "  else: # 홀수면 그에 맞게 나눔\n",
        "    df1 = df[df['label']==1].sample(n=split_size+1)\n",
        "    df2 = df[df['label']==0].sample(n=split_size) \n",
        "  sample = pd.concat([df1,df2], axis=0)\n",
        "  return sample\n",
        "\n",
        "sample_df = label_evenly_balanced_dataset_sampler(df, n_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXLT6tAdaA34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "447d4fe3-bdf6-4978-948b-b209439bc3b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    500\n",
              "0    500\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ],
      "source": [
        "# 검증\n",
        "\n",
        "sample_df.label.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLNUjgawLnJ1"
      },
      "source": [
        "### CustomClassifier 클래스 구현\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week2/week2_2_bertclf.png?raw=true\" width=400>\n",
        "\n",
        "- 그림과 같이 사전 학습(pre-trained)된 `BERT` 모델을 불러와 그 위에 **1 hidden layer**와 **binary classifier layer**를 쌓아 fine-tunning 모델을 생성할 것이다.    \n",
        "---\n",
        "- hidden layer 1개와 output layer(binary classifier layer)를 갖는 `CustomClassifier` 클래스를 구현하라.\n",
        "- 클래스 정의\n",
        "  - 생성자 입력 매개변수\n",
        "    - `hidden_size` : BERT의 embedding size\n",
        "    - `n_label` : class(label) 개수\n",
        "  - 생성자에서 생성할 변수\n",
        "    - `bert` : BERT 모델 인스턴스 \n",
        "    - `classifier` : 1 hidden layer + relu +  dropout + classifier layer를 stack한 `nn.Sequential` 모델\n",
        "      - 첫번재 히든 레이어 (첫번째 `nn.Linear`)\n",
        "        - input: BERT의 마지막 layer의 1번재 token ([CLS] 토큰) (shape: `hidden_size`)\n",
        "        - output: (shape: `linear_layer_hidden_size`)\n",
        "      - 아웃풋 레이어 (두번째 `nn.Linear`)\n",
        "        - input: 첫번째 히든 레이어의 아웃풋 (shape: `linear_layer_hidden_size`)\n",
        "        - output: target/label의 개수 (shape:2)\n",
        "  - 메소드\n",
        "    - `forward()`\n",
        "      - BERT output에서 마지막 레이어의 첫번째 토큰 ('[CLS]')의 embedding을 가져와 `self.classifier`에 입력해 아웃풋으로 logits를 출력함.\n",
        "  - 주의 사항\n",
        "    - `CustomClassifier` 클래스는 부모 클래스로 `nn.Module`을 상속 받는다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U0WbqVv62Zvy"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Im98H4-U1eQQ"
      },
      "outputs": [],
      "source": [
        "# classifier 구현\n",
        "class CustomClassifier(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size: int, n_label: int):\n",
        "    super(CustomClassifier, self).__init__()\n",
        "\n",
        "    self.bert = BertModel.from_pretrained(\"klue/bert-base\")\n",
        "\n",
        "    dropout_rate = 0.1\n",
        "    linear_layer_hidden_size = 32\n",
        "\n",
        "    # torch.nn에서 제공되는 Sequential, Linear, ReLU, Dropout 함수 활용\n",
        "    self.classifier = nn.Sequential(\n",
        "          nn.Linear(hidden_size, linear_layer_hidden_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(),\n",
        "          nn.Linear(linear_layer_hidden_size,n_label)\n",
        "        )\n",
        "\n",
        "  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
        "\n",
        "    outputs = self.bert(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids,\n",
        "    )\n",
        "\n",
        "    # BERT 모델의 마지막 레이어의 첫번재 토큰을 인덱싱\n",
        "    cls_token_last_hidden_states = outputs['pooler_output'] # 마지막 layer의 첫 번째 토큰 (\"[CLS]\") 벡터를 가져오기, shape = (1, hidden_size)\n",
        "    # print(cls_token_last_hidden_states.shape)\n",
        "    logits = self.classifier(cls_token_last_hidden_states)\n",
        "    # print(cls_token_last_hidden_states)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x7PU1t1LnJ1"
      },
      "source": [
        "## Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXesCG5TLnJ1"
      },
      "source": [
        "### 학습 데이터를 배치 단위로 저장하는 이터레이터 함수 `data_iterator` 구현\n",
        "- 데이터 프레임을 입력 받아 text를 \b토큰 id로 변환하고 label은 텐서로 변환해 배치만큼 잘라 (input, \btarget) 튜플 형태의 이터레이터를 생성하는 `data_iterator` 함수를 구현하라.\n",
        "- 함수 정의 \n",
        "  - 입력 매개변수\n",
        "    - `input_column` : text 데이터 column 명\n",
        "    - `target_column` : label 데이터 column 명\n",
        "    -  `batch_size` : 배치 사이즈\n",
        "  - 조건\n",
        "    - 함수는 다음을 수행해야 함 \n",
        "      - 데이터 프레임 랜덤 셔플링\n",
        "      - `tokenizer_bert`로 text를 token_id로 변환 + 텐서화 \n",
        "      - target(label)을 텐서화\n",
        "  - 반환값 \n",
        "    - (input, target) 튜플 형태의 이터레이터를 반환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-tJERGI4Fzk",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cedecd64-db67-4a11-ed96-d0cb7b4bbc8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.16.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.47)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "#!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlcYCOyW3d2t"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_U_c-Mf3d2t"
      },
      "outputs": [],
      "source": [
        "tokenizer_bert = BertTokenizer.from_pretrained(\"klue/bert-base\") # lower-cased version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2VnIY-ALnJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9911f807-a40d-4f1a-d321-0e4690dc3413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence: 죄의식을 아름답게 치유하는 호세의 모습에서 예수가 생각났다.\n",
            "\n",
            "Tokenized Sentence: {'input_ids': tensor([[    2, 25857,  2069,  7524,  2318,  7935,  2205,  2259, 21267,  2079,\n",
            "          3781, 27135,  5558,  2116, 18958,  2062,    18,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "# 토크나이징 예시 (1개의 문장)\n",
        "\n",
        "# 1. string type의 문장을 가져옴\n",
        "ex_sent = sample_df.document.iloc[0]\n",
        "print(f\"Original Sentence: {ex_sent}\\n\")\n",
        "\n",
        "# 2. 문장을 토크나이즈 함. 이 때, 특수 토큰 (\"[CLS]\", \"[SPE]\")을 자동으로 추가하고 pytorch의 tensor형태로 변환해 반환함\n",
        "tensor_sent = tokenizer_bert(\n",
        "    ex_sent,\n",
        "    add_special_tokens=True, # 문장의 앞에 문장 시작을 알리는 \"[CLS]\"토큰, 문장의 끝에 문장 끝을 알리는 \"[SPE]\"토큰을 자동으로 추가\n",
        "    return_tensors='pt' # pytorch tensor로 반환할 것\n",
        ")\n",
        "print(f\"Tokenized Sentence: {tensor_sent}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtXP_wRFLnJ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5bea974-e424-4290-9453-321fa5e990fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Sentence 1: 죄의식을 아름답게 치유하는 호세의 모습에서 예수가 생각났다.\n",
            "Original Sentence 2: 인간이란 어떤 존재인지 알고 싶다면 도그빌을 보면 된다. 뉴욕 브로드웨이의 연극을 눈앞에서 보기 어려운 이들에게, 설사 본다해도 이렇게 가까운 화면으로는 보기 어렵기에 연기를 감상하는 것만으로도 큰 재미!! 연기를 자세히 보세요. 꿀잼!\n",
            "\n",
            "Tokenized Sentence list: {'input_ids': tensor([[    2, 25857,  2069,  7524,  2318,  7935,  2205,  2259, 21267,  2079,\n",
            "          3781, 27135,  5558,  2116, 18958,  2062,    18,     3,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [    2,  3804,  2052,  2241,  3711,  3990,  8969, 22819,  1335,  5209,\n",
            "           848,  2029,  2548,  2069,  1160,  2460,  3622,    18,  5372, 29262,\n",
            "          2079,  5909,  2069,  7239, 27135,  1160,  2015,  4536,  1504,  2031,\n",
            "          2170,  2318,    16, 10029,  4919,  2097,  2119,  3737,  5376,  5995,\n",
            "          6233,  2259,  1160,  2015,  4258, 12551,  4483,  2138,  7397,  2205,\n",
            "          2259,   575,  2154,  6233,  2119,  1751,  4697,     5,     5,  4483,\n",
            "          2138,  7505, 31153,  2182,    18,   692,  3468,     5,     3]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
            "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ],
      "source": [
        "# 토크나이징 예시 (2개의 문장)\n",
        "\n",
        "# 1. 2개의 문장을 가진 list 생성\n",
        "ex_sent_list = list(sample_df.document.iloc[:2].values)\n",
        "for i, sent in enumerate(ex_sent_list):\n",
        "    print(f\"Original Sentence {i+1}: {sent}\")\n",
        "\n",
        "# 2. 문장 리스트를 토크나이즈 함. 이 때, 리스트 내 문장들의 토큰 길이가 동일할 수 있도록 가장 긴 문장을 기준으로 부족한 위치에 \"[PAD]\" 토큰을 추가\n",
        "tensor_sent_list = tokenizer_bert(\n",
        "    ex_sent_list,\n",
        "    add_special_tokens=True,\n",
        "    return_tensors='pt',\n",
        "    padding=\"longest\" # 가장 긴 문장을 기준으로 token개수를 맞춤. 모자란 토큰 위치는 \"[PAD]\" 토큰을 추가\n",
        ")\n",
        "\n",
        "print(f\"\\nTokenized Sentence list: {tensor_sent_list}\")\n",
        "\n",
        "# 토크나이즈 된 두 문장의 길이가 동일함을 검증\n",
        "assert tensor_sent_list['input_ids'][0].shape == tensor_sent_list['input_ids'][1].shape "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tR22xs-xf1QH"
      },
      "outputs": [],
      "source": [
        "def data_iterator(df, input_column, target_column, batch_size):\n",
        "  \"\"\"\n",
        "  데이터 프레임을 셔플한 후 \n",
        "  데이터 프레임의 input_column을 batch_size만큼 잘라 토크나이즈 + 텐서화하고, target_column을 batch_size만큼 잘라 텐서화 하여\n",
        "  (input, output) 튜플 형태의 이터레이터를 생성\n",
        "  \"\"\"\n",
        "\n",
        "  global tokenizer_bert\n",
        "\n",
        "  # 1. 데이터 프레임 셔플\n",
        "  #    pandas의 sample 함수 사용\n",
        "  df = df.sample(frac=1,random_state = 101) # freeze/unfreeze 비교\n",
        "\n",
        "  # 2. 이터레이터 생성\n",
        "  for idx in range(0, df.shape[0], batch_size):\n",
        "    batch_df = df[idx:idx+batch_size] # batch_size만큼 데이터 추출\n",
        "    \n",
        "    tensorized_input = tokenizer_bert(\n",
        "        list(batch_df[input_column]),\n",
        "        add_special_tokens=True,\n",
        "        return_tensors='pt',\n",
        "        padding=\"longest\"\n",
        "\n",
        "    ) # df의 text를 토크나이징 + token id로 변환 + 텐서화 (df의 input_column 사용)\n",
        "    \n",
        "    tensorized_target = torch.tensor(list(batch_df[target_column])) # target(label)을 텐서화 (df의 target_column 사용)\n",
        "    \n",
        "    yield tensorized_input, tensorized_target # 튜플 형태로 yield"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTlAV0hqILmc"
      },
      "outputs": [],
      "source": [
        "batch_size=32\n",
        "train_iterator = data_iterator(sample_df, 'document', 'label', batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9VNAMchf1QI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71429c2d-7c56-4a57-d2fd-a3abe3657979"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[   2, 4825, 2069,  ...,    0,    0,    0],\n",
              "        [   2, 4510, 3771,  ...,    0,    0,    0],\n",
              "        [   2, 5725, 5643,  ...,    0,    0,    0],\n",
              "        ...,\n",
              "        [   2, 3949, 2706,  ...,    0,    0,    0],\n",
              "        [   2,   24, 2532,  ...,    0,    0,    0],\n",
              "        [   2, 1504, 3771,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0],\n",
              "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0],\n",
              "        [1, 1, 1,  ..., 0, 0, 0]])}"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ],
      "source": [
        "next(train_iterator)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cqnp2Q6ZLnJ2"
      },
      "source": [
        "## Advanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQVTqAUxLnJ2"
      },
      "source": [
        "### `data_iterator` 함수로 생성한 이터레이터를 for loop 돌면서 배치 단위의 데이터를 모델에 학습하는 `train()` 함수 구현\n",
        "- 함수 정의\n",
        "  - 입력 매개변수\n",
        "    - `model` : BERT + 1 hidden layer classifier 모델\n",
        "    - `data_iterator` : train data iterator\n",
        "- Reference\n",
        "  - [Loss: CrossEntropyLoss official document](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
        "  - [Optimizer: AdamW official document](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-sE7xjYcRD1p"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from numpy.core.fromnumeric import nonzero"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Er1qKtsf1QJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33d7b9a2-1018-4fa9-ea4b-999101be3b4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# 모델 클래스 정의\n",
        "model = CustomClassifier(hidden_size=768, n_label=2)\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# 데이터 이터레이터 정의 \n",
        "train_iterator = data_iterator(sample_df, 'document', 'label', batch_size)\n",
        "\n",
        "# 로스 및 옵티마이저\n",
        "loss_fct = CrossEntropyLoss()\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=2e-5,\n",
        "    eps=1e-8\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZvY5rxDKHQAp"
      },
      "outputs": [],
      "source": [
        "def train(model, data_iterator):\n",
        "\n",
        "  global loss_fct # 위에서 정의한 loss 함수\n",
        "\n",
        "  # 배치 단위 평균 loss와 총 평균 loss 계산하기위해 변수 생성\n",
        "  total_loss, batch_loss, batch_count = 0,0,0\n",
        "  \n",
        "  # model을 train 모드로 설정 & device 할당\n",
        "  model.train()\n",
        "  model.to(device)\n",
        "  \n",
        "  # data iterator를 돌면서 하나씩 학습\n",
        "  for step, batch in enumerate(data_iterator):\n",
        "    batch_count+=1\n",
        "    \n",
        "    # tensor 연산 전, 각 tensor에 device 할당\n",
        "    batch = tuple(item.to(device) for item in batch)\n",
        "    \n",
        "    batch_input, batch_label = batch\n",
        "    \n",
        "    # batch마다 모델이 갖고 있는 기존 gradient를 초기화\n",
        "    model.zero_grad()\n",
        "    \n",
        "    # forward\n",
        "    logits = model.forward(batch_input['input_ids'],batch_input['attention_mask'],batch_input['token_type_ids'] ) \n",
        "    \n",
        "    # loss\n",
        "    loss = loss_fct(logits, batch_label)\n",
        "    batch_loss += loss.item()\n",
        "    total_loss += loss.item()\n",
        "    \n",
        "    # backward -> 파라미터의 미분(gradient)를 자동으로 계산\n",
        "    loss.backward()\n",
        "    \n",
        "    # optimizer 업데이트\n",
        "    optimizer.step()\n",
        "      \n",
        "    # 배치 10개씩 처리할 때마다 평균 loss를 출력\n",
        "    if (step % 10 == 0 and step != 0):\n",
        "      print(f\"Step : {step}, Avg Loss : {batch_loss / batch_count:.4f}\")\n",
        "      \n",
        "      # 변수 초기화 \n",
        "      batch_loss, batch_count = 0,0\n",
        "  \n",
        "  print(f\"Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "  print(\"Train Finished\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEYo8z9RLnJ2"
      },
      "source": [
        "### 지금까지 구현한 함수와 클래스를 모두 불러와 `train()` 함수를 실행하자\n",
        "- fine-tuning 모델 클래스 (`CustomClassifier`)\n",
        "    - hidden_size = 768\n",
        "    - n_label = 2\n",
        "- 데이터 이터레이터 함수 (`data_iterator`)\n",
        "    - batch_size = 32\n",
        "- loss \n",
        "    - `CrossEntropyLoss()`\n",
        "- optimizer\n",
        "    - optimizer는 loss(오차)를 상쇄하기 위해 파라미터를 업데이트 하는 과정\n",
        "    - `optimizer.step()` 시 파라미터가 업데이트 됨 \n",
        "    - lr = 2e-5\n",
        "- Reference\n",
        "  - [Optimizer 종류 설명 한국어 블로그 ](https://ganghee-lee.tistory.com/24)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCODIxCfFEDP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2559df2-e1ca-4609-9384-a6b5c648b1dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step : 10, Avg Loss : 0.6879\n",
            "Step : 20, Avg Loss : 0.6205\n",
            "Step : 30, Avg Loss : 0.5613\n",
            "Mean Loss : 0.6191\n",
            "Train Finished\n"
          ]
        }
      ],
      "source": [
        "# 학습 시작\n",
        "train(model, train_iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37UbAkh7LnJ3"
      },
      "source": [
        "## fine-tuning 2가지 방법론 비교\n",
        "- pre-trained BERT 모델 파라미터를 **freeze**한 채 학습하라\n",
        "    - BERT의 파라미터의 `requires_grad` 값을 `False`로 바꾸면, 학습 시 BERT의 파라미터는 미분이 계산되지도, 업데이트 되지도 않는다. \n",
        "    - 이렇게 특정 모델의 파라미터가 업데이트 하지 못하도록 설정하는 것을 **freeze**라고 한다. \n",
        "    - BERT 파라미터를 freeze시킨 채 학습을 진행해보자. 이럴 경우, 우리가 직접 쌓은 fine-tuning layer의 파라미터만 업데이트 된다. \n",
        "- **unfreeze**와 **freeze** 모델의 성능을 비교해 보자. 어떤 방식이 더 우수한가?\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld260K3YLnJ3"
      },
      "outputs": [],
      "source": [
        "class CustomClassifierFreezed(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size: int, n_label: int):\n",
        "    super(CustomClassifierFreezed, self).__init__()\n",
        "\n",
        "    self.bert = BertModel.from_pretrained(\"klue/bert-base\")\n",
        "    # freeze BERT parameter\n",
        "    # BERT의 파라미터는 고정값으로 두고 BERT 위에 씌운 linear layer의 파라미터만 학습하려고 한다. \n",
        "    # 이 경우, BERT의 파라미터의 'requires_grad' 값을 False로 변경해줘야 학습 시 해당 파라미터의 미분값이 계산되지 않는다.\n",
        "    for param in self.bert.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    dropout_rate = 0.1\n",
        "    linear_layer_hidden_size = 32\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "          nn.Linear(hidden_size, linear_layer_hidden_size),\n",
        "          nn.ReLU(),\n",
        "          nn.Dropout(),\n",
        "          nn.Linear(linear_layer_hidden_size,n_label)\n",
        "          )\n",
        "\n",
        "      \n",
        "  def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
        "\n",
        "    outputs = self.bert(\n",
        "        input_ids,\n",
        "        attention_mask=attention_mask,\n",
        "        token_type_ids=token_type_ids,\n",
        "    )\n",
        "    \n",
        "    # BERT 모델의 마지막 레이어의 첫번재 토큰을 인덱싱\n",
        "    cls_token_last_hidden_states = outputs['pooler_output'] # 마지막 layer의 첫 번째 토큰 (\"[CLS]\") 벡터를 가져오기, shape = (1, hidden_size)\n",
        "    logits = self.classifier(cls_token_last_hidden_states)\n",
        "\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ClEEHB6F6LW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f3cc8df-2728-45dc-eb54-b4528e4a676f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ],
      "source": [
        "# freeze 모델\n",
        "# model을 제외한 설정값은 \b위에서 실행한 unfreeze 모델과 동일\n",
        "model = CustomClassifierFreezed(hidden_size=768,n_label=2)\n",
        "\n",
        "# 데이터 이터레이터\n",
        "batch_size = 32 \n",
        "train_iterator = data_iterator(sample_df, 'document', 'label', batch_size)\n",
        "\n",
        "# 로스 및 옵티마이저\n",
        "loss_fct = CrossEntropyLoss()\n",
        "optimizer = AdamW(\n",
        "    model.parameters(),\n",
        "    lr=2e-5,\n",
        "    eps=1e-8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqcPcWZeLnJ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5dff733-d1ce-43a7-9d75-e1cc11c18ee2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step : 10, Avg Loss : 0.7269\n",
            "Step : 20, Avg Loss : 0.7056\n",
            "Step : 30, Avg Loss : 0.6999\n",
            "Mean Loss : 0.7138\n",
            "Train Finished\n"
          ]
        }
      ],
      "source": [
        "# 학습 시작\n",
        "train(model, train_iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 회고🧐\n",
        "\n",
        "1. ` 데이터` : 전처리 및 `label을 균등`하게 추출하는 방법을 시도해 보았다. \n",
        "2. `모델` : \\__init\\__ 메소드와 forward 메소드로 pretrained model(bert)의 fine tuning을 했다. (CLS token을 활용한 binary classification)\n",
        "3. `데이터 추출` : bert의 tokenizer로 training data(3겹의 embedding input)와 label을 batch 단위로 나누어서 입력할 수 있도록 설계했다.\n",
        "4. `metric` : `loss`(CrossEntropy)와 `optimizer`(AdamW)를 설정하고 학습을 진행하였다. \n",
        "5.`freezing` : bert model의 parameter가 update되지 않도록 `freeze`하고 결과를 비교해 보았다. 성능이 0.1 정도 개선되고 시간도 물론 개선되었다. 시간이 개선된 것은 gradient를 업데이트 하는 과정이 없었기에 충분히 납득할 수 있지만, 성능이 왜 나아졌을까? 생각을 해보아야 할 문제이다.\n"
      ],
      "metadata": {
        "id": "3-Qj45PEXyIW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oZc3zaSGctPL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Jesung Ryu - Week2_2_assignment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}