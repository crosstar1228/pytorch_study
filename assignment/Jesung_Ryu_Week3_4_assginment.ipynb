{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592U6lXs3d2t"
      },
      "source": [
        "# Week3_4 Assignment\n",
        "\n",
        "## [BASIC](#Basic) \n",
        "- Encoder & Decoder Layer 코드를 직접 필사하고 각 함수에 주석을 달 수 있다. \n",
        "\n",
        "## [CHALLENGE](#Challenge)\n",
        "- 텐서의 크기(shape)를 계산할 수 있다. \n",
        "\n",
        "## [ADVANCED](#Advanced)\n",
        "- 완성된 transformer 모델의 모든 학습 가능한 파라미터 이름과 크기(shape)를 출력할 수 있다.\n",
        "\n",
        "### Informs\n",
        "논문 정리 블로그 : [Transformer 논문 제대로 읽기](https://velog.io/@crosstar1228/NLP-Attention-is-all-you-need-Transformer-%EB%85%BC%EB%AC%B8-%EC%A0%9C%EB%8C%80%EB%A1%9C-%EC%9D%BD%EA%B8%B0)\n",
        "이번 과제에서는 \"[Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)\"의 코드를 필사해본다.   \n",
        "\"Annotated Transformer\"는 \"Attention is all you need\" 논문에서 제안한 transformer 모델을 pytorch 라이브러리로 직접 구현한다.   \n",
        "코드 필사를 통해 다음을 배울 수 있다.    \n",
        "- Encoder, Decoder 구조\n",
        "- Attention Mechanism\n",
        "- \"residual connection\", \"layer normalization\" 등의 구조 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoebvnNZ99r-"
      },
      "source": [
        "코드 필사를 시작하기 앞서, transformer 모델의 최종 구조를 살펴보자.    \n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_full.png?raw=true\" width=\"500\" align=\"center\"/>\n",
        "\n",
        "최종 모델은 `EncoderDecoder()` 클래스에 여러 인스턴스를 생성자의 입력 파라미터로 넣어 생성한다.    \n",
        "앞으로 우리는 `EncoderDecoder()` 클래스와 같은 여러 클래스들을 구현하고 연결할 것이다. 따라서 대략적인 클래스간의 관계를 살펴보고 이해한다면 보다 큰 그림을 가지고 코드 필사를 할 수 있을 것이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DB6cNaXP99sB"
      },
      "source": [
        "Transformer 모델은 크게 4가지 클래스로 구현된다.    \n",
        "- Frame\n",
        "    - frame 역할을 하는 `EncoderDecoder` 클래스\n",
        "- Input Embedding & Encoding\n",
        "    - 입력값을 벡터화하는 `Embeddings`, `PositionalEncoding`\n",
        "- Encoder & Decoder\n",
        "    - 각 6개 layer를 갖고 있는 `Encoder`, `Decoder`\n",
        "    - layer 1층을 구현한 `EncoderLayer`, `DecoderLayer`\n",
        "- Sublayer\n",
        "    - `EncoderLayer`, `DecoderLayer` 내부에서 사용되는 Sublayer 클래스인 `MultiHeadAttiontion`, `PositionwiseFeedForward`\n",
        "    - Sublayer 클래스들을 연결하는 `SublayerConnection`\n",
        "    \n",
        "아래 좌측 도식에서 각 클래스의 색상은 아래 우측 도식(transformer 구조)의 색상과 맵핑되어 있다.    \n",
        "각 클래스의 역할과 클래스 간 연결 관계를 생각하면서 transformer를 코드로 구현해보자.   \n",
        "\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_map.png?raw=true\" width=\"400\" height=\"400\" align=\"left\"/>\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_transformer.png?raw=true\" width=\"300\" height=\"400\" align=\"right\"/>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qaadVYo799sE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import math, copy, time\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OEO0al299sJ"
      },
      "source": [
        "## Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKKyKfqB99sL"
      },
      "source": [
        "### Frame\n",
        "- `EncoderDecoder`\n",
        "\n",
        "아래 도식은 `EncoderDecoder` 클래스의 `forward()`, `encode()`, `decode()` 메소드를 도식화 한 것이다.    \n",
        " \n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_encoderdecoder.png?raw=true\" width=500>\n",
        "\n",
        "\n",
        "- `Generator`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MECCTGpt99sP"
      },
      "outputs": [],
      "source": [
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "    '''\n",
        "     transformer encoder-deocoder 구조의 뼈대입니다.\n",
        "    '''\n",
        "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
        "      super(EncoderDecoder, self).__init__()\n",
        "      self.encoder = encoder\n",
        "      self.decoder = decoder\n",
        "      self.src_embed = src_embed # source를 input으로 받아 embedding 으로 바꿔줌\n",
        "      self.tgt_embed = tgt_embed # target를 input으로 받아 embedding 으로 바꿔줌\n",
        "      self.generator = generator \n",
        "    \n",
        "    # encoding 및 decoding 전 과정 forward propagation\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
        "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask) \n",
        "    \n",
        "    # encoder에 source 와 masking 정보를 input으로\n",
        "    def encode(self, src, src_mask):\n",
        "        return self.encoder(self.src_embed(src), src_mask)  \n",
        "    \n",
        "    # decoder에 encoder output, source mask 및 target embedding과 target masking 정보 넣음 \n",
        "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
        "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Py2wcYPX99sT"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, vocab):\n",
        "      super(Generator, self).__init__()\n",
        "      self.proj = nn.Linear(d_model, vocab) # d_model 차원의 모델의 embedding vector를 통해 vocab 차원의 output값 생성\n",
        "    \n",
        "    # vocab 개수만큼의 output 값들을 모아 softmax로 확률 계산\n",
        "    def forward(self, x):\n",
        "      return F.log_softmax(self.proj(x), dim=-1) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xI-5SRHD99sX"
      },
      "source": [
        "### Encoder\n",
        "- `Encoder`\n",
        "- `EncoderLayer`\n",
        "- `SublayerConnection`\n",
        "- Reference\n",
        "    - Layer Normalization\n",
        "        - [한국어 설명](https://yonghyuc.wordpress.com/2020/03/04/batch-norm-vs-layer-norm/)\n",
        "        - [torch official docs](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)\n",
        "    - Residual Connection\n",
        "        - [한국어 설명](https://itrepo.tistory.com/36)\n",
        "    - pytorch ModuleList\n",
        "        - [torch official docs](https://pytorch.org/docs/1.9.1/generated/torch.nn.ModuleList.html)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DjIjUBjN99sc"
      },
      "outputs": [],
      "source": [
        "# N개의 module 생성. default 값 6 (class 아닌 함수)\n",
        "def clones(module, N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wgglBAyM99se"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layer, N):\n",
        "      super(Encoder,self).__init__()\n",
        "      self.layers = clones(layer, N)\n",
        "      self.norm = LayerNorm(layer.size) # layer size는 곧 feature의 개수\n",
        "    \n",
        "    # layer별로 input과 masking 정보를 통과시킴\n",
        "    def forward(self, x, mask):\n",
        "      for layer in self.layers:\n",
        "        x = layer(x, mask) \n",
        "      return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AvGP8Gsd99sg"
      },
      "outputs": [],
      "source": [
        "# 데이터 별 feature들을 normalization 해주는 함수 \n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \n",
        "    # feature들이 list를 \n",
        "    def __init__(self, features, eps=1e-6):\n",
        "      super(LayerNorm,self).__init__()\n",
        "      self.a_2 = nn.Parameter(torch.ones(features))\n",
        "      self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "      self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "      # 마지막 차원에 feature들에 해당하는 값\n",
        "      mean = x.mean(-1, keepdim=True)\n",
        "      std = x.std(-1, keepdim=True)\n",
        "      return self.a_2 * (x - mean) / (std + self.eps) + self.b_2 # torch.ones 에 standardization 된 값들을 곱하고, 영벡터 더함\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "525_O3YE99si"
      },
      "outputs": [],
      "source": [
        "class SublayerConnection(nn.Module):\n",
        "    \n",
        "    def __init__(self, size, dropout):\n",
        "      super(SublayerConnection, self).__init__()\n",
        "      self.norm = LayerNorm(size)\n",
        "      # 각 sub-layer의 output에 dropout 적용(Residual computation, 그러니까 input과 더해지고 layer normalize 되는 연산을 진행하기 전에).\n",
        "      self.dropout = nn.Dropout(dropout)\n",
        "    #  layernorm( x + F(x))\n",
        "    def forward(self, x, sublayer):\n",
        "      return x + self.dropout(sublayer(self.norm(x)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LlGCPEVp99sk"
      },
      "outputs": [],
      "source": [
        "# attention , ffw layer + connection\n",
        "class EncoderLayer(nn.Module):\n",
        "    \n",
        "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
        "      super(EncoderLayer, self).__init__()\n",
        "      self.self_attn = self_attn\n",
        "      self.feed_forward = feed_forward\n",
        "      self.sublayer = clones(SublayerConnection(size, dropout), 2) # size 만큼의 layer normalization\n",
        "      self.size = size\n",
        "        \n",
        "    def forward(self, x, mask):\n",
        "      x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask)) # self attention -> query key value 모두 자체적인 output\n",
        "      return self.sublayer[1](x, self.feed_forward) # masking"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOiYmYWc99sm"
      },
      "source": [
        "### Decoder\n",
        "- `Decoder`\n",
        "- `DecoderLayer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Ik47frFO99so"
      },
      "outputs": [],
      "source": [
        "# 6개이고 각 layer 별로 3개의 sub layer\n",
        "# masked multihead self attention + multihead attention of encoder output+ position-wise FC FFN\n",
        "class Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, layer, N):\n",
        "      super(Decoder, self).__init__()\n",
        "      self.layers = clones(layer, N)\n",
        "      self.norm = LayerNorm(layer.size)\n",
        "    \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "      for layer in self.layers:\n",
        "            x = layer(x, memory, src_mask, tgt_mask)\n",
        "      return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ElsG9P7M99sq"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
        "      super(DecoderLayer, self).__init__()\n",
        "      self.size = size\n",
        "      self.self_attn = self_attn\n",
        "      self.src_attn = src_attn\n",
        "      self.feed_forward = feed_forward\n",
        "      self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
        "    \n",
        "    \n",
        "    def forward(self, x, memory, src_mask, tgt_mask):\n",
        "      m = memory\n",
        "      # query, key, value\n",
        "      x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask)) \n",
        "      # query가 이전의 decoder layer로부터 오고, key와 value는 encoder의 output으로부터 옴\n",
        "      # query 는 x      \n",
        "      # m, m 은 각각 encoder output의 key, value\n",
        "      x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
        "      return self.sublayer[2](x, self.feed_forward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhPP8LVw99sr"
      },
      "source": [
        "### Sublayer\n",
        "- `attention` 함수\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_attention.png?raw=true\" width=\"500\" align=\"center\"/>  \n",
        "\n",
        "- `MultiHeadedAttention`\n",
        "- `PositionwiseFeedForward`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7o1-iOBu99ss"
      },
      "source": [
        "### Challenge\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0ochH0n99st"
      },
      "source": [
        "### Q1. 위 도식에 따라 `score`, `p_attn`, `attention` 을 구하라 \n",
        "$$attention(Q,K,V) = softmax( \\frac {QK^T} {\\sqrt{d_k}}) V$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZMYcy8h499sv"
      },
      "outputs": [],
      "source": [
        "def attention(query, key, value, mask=None, dropout=None):\n",
        "  d_k = query.size(-1)\n",
        "  # matrix multiplication( -2, -1 차원을 transpose)\n",
        "  scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "  # 0으로 masking\n",
        "  if mask is not None: \n",
        "  # 해당 지점 0으로 마스킹\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "  p_attn = F.softmax(scores, dim = -1)\n",
        "  # dropout layer 통과\n",
        "  if dropout is not None:\n",
        "      p_attn = dropout(p_attn)\n",
        "  return torch.matmul(p_attn, value), p_attn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x25aeigL99sw"
      },
      "source": [
        "###Q2. query, key, value가 모두 (m, d_k) shape의 matrix라고 가정할 때, `score`, `p_attn`, `attention`의 shape을 각각 구하라\n",
        "\n",
        "- `m` : input sequence token 개수\n",
        "- `d_k` : embedding vector의 차원/head\n",
        "---\n",
        "- score : (`m` , `m`)\n",
        "- p_attn : (`m`, `m`)\n",
        "- attention : (`m`, `d_k`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHfxLJKz99sx"
      },
      "source": [
        "### (아래의 **Q3을 먼저 풀고 돌아오세요**) Q4.  query, key, value가 모두 (12, 8, 1, 64) shape의 tensor라고 가정할 때 , `score`, `p_attn`, `attention`의 shape을 각각 구하라\n",
        "\n",
        "- score : (12,8,1,1)\n",
        "- p_attn : (12,8,1,1)\n",
        "- attention : (12,8,1,64)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYnffQE799sy"
      },
      "source": [
        "- `MultiHeadedAttention`\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_multihead.png?raw=true\" width=\"300\" align=\"center\"/>  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://images.velog.io/images/crosstar1228/post/3db389a4-a792-4c58-a400-fb97ac3ec89d/image.png)\n",
        "- $$W_i^Q, W_i^K, W_i^V$$ 는 각각 query, key, value의 projection 된 parameter matrix\n",
        "- $$W_i^Q \\in R^{d_{model} * d_k}$$, $$W_i^K \\in R^{d_{model} * d_k}$$, $$W_i^V \\in R^{d_{model} * d_v}$$\n",
        "- $$W^o \\in R^{{h d_v}* d_{model}}$$ : head 개수 만큼 concat 했으므로"
      ],
      "metadata": {
        "id": "dhiPsDMzroFB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "uhFKlJ2b99sz"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_model, dropout=0.1):\n",
        "      super(MultiHeadedAttention, self).__init__()\n",
        "      assert d_model % h == 0\n",
        "      # d_v == d_k 가정\n",
        "      self.d_k = d_model // h\n",
        "      self.h = h\n",
        "      self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
        "      self.attn = None\n",
        "      self.dropout = nn.Dropout(p=dropout)\n",
        "    \n",
        "    \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "      if mask is not None:\n",
        "          # 같은 mask가 모든 h값에 매핑\n",
        "          mask = mask.unsqueeze(1) \n",
        "      nbatches = query.size(0) # 0번째 축은 batch size\n",
        "\n",
        "      # 1) (n_batches, h, 1, d_k) 모양으로 project \n",
        "      query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
        "                              for l, x in zip(self.linears, (query, key, value))]\n",
        "        \n",
        "      # 2) attention 연산 수행\n",
        "      x, self.attn = attention(query, key, value, mask=mask,dropout=self.dropout)\n",
        "        \n",
        "      # 3) view 이용하여 concat \n",
        "      x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
        "      return self.linears[-1](x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M46Ensa499s0"
      },
      "source": [
        "### Q3.  query, key, value가 모두 (12, 512) shape의 matrix이고, h 값이 8 이라고 가정할 때, 아래 값의 shape을 각각 구하라\n",
        "\n",
        "- `d_k` (d_k = d_model // h) : 512/8  = 64\n",
        "- `nn.Linear(d_model, d_model)(query)` : (12,512(d_model))\n",
        "- `nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, d_k)` : (12, 1, 8, 64)\n",
        "- `nn.Linear(d_model, d_model)(query).view(nbatches, -1, h, d_k).transpose(1,2)` : (12, 8, 1, 64) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twZoeFr799s1"
      },
      "source": [
        "- `PositionwiseFeedForward`\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_pwff.png?raw=true\" width=\"300\" align=\"center\"/>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "nZzpucvQ99s2"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "  def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff) # input size d_model\n",
        "        self.w_2 = nn.Linear(d_ff, d_model) # hidden size d_ff\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "      return self.w_2(self.dropout(F.relu(self.w_1(x)))) #w_1 -> relu -> dropout -> w_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqjsUsbu99s3"
      },
      "source": [
        "### Input Embedding & Encoding\n",
        "- `Embeddings`\n",
        "    - [pytorch official docs](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "FBVJFurO99s3"
      },
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "     # vocab, d_model 의 모양의 lookup table 생성\n",
        "    def __init__(self, d_model, vocab):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.lut = nn.Embedding(vocab, d_model)\n",
        "        self.d_model = d_model\n",
        "    # lookup table * sqrt(d_model)\n",
        "    def forward(self, x):\n",
        "        return self.lut(x) * math.sqrt(self.d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Po31qs_A99s5"
      },
      "source": [
        "- `PositionalEncoding`\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_pe.png?raw=true\" width=\"500\" align=\"center\"/>  \n",
        "\n",
        "- `position` 변수 설명\n",
        "    - 모든 position (=최대 토큰 개수)의 값을 갖고 있는 matrix\n",
        "- `div_term` 변수 설명\n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_div.png?raw=true\" width=\"500\" align=\"center\"/>  \n",
        "- `Embedding` + `Encoding` 도식화 \n",
        "\n",
        "<img src=\"https://github.com/ChristinaROK/PreOnboarding_AI_assets/blob/36a670a7b6233d5218a495150beb337a899ecb70/week3/week3_3_emb_enc.png?raw=true\" width=\"400\" align=\"center\"/>  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "RP-_an3x99s5"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        # positional encoding 연산\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    # gradient 계산이 가능하게 만들어 준 후 input embedding 값에 더해줌    \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNf13Gkm99s6"
      },
      "source": [
        "### Q4.  max_len이 512이고, d_model이 512라고 가정할 때, `position`과 `div_term`의 shape을 구하라\n",
        "\n",
        "- `position` : (512,1)\n",
        "- `div_term` : (256,)\n",
        "- `position * div_term` : (512,256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rri-daP399s7"
      },
      "source": [
        "### Advanced"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3ZixTN199s8"
      },
      "source": [
        "### Finally Build Model\n",
        "- Xavier Initialization\n",
        "    - [한국어 자료](https://huangdi.tistory.com/8)\n",
        "    - [pytorch official docs](https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.xavier_uniform_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "kPdGsCiC99s8"
      },
      "outputs": [],
      "source": [
        "def make_model(src_vocab, tgt_vocab, \n",
        "               N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
        "  c = copy.deepcopy\n",
        "  attn = MultiHeadedAttention(h, d_model)  # d_k 차원의 h개의 head로 나누어 attention 연산 수행\n",
        "  ff = PositionwiseFeedForward(d_model, d_ff, dropout) # d_model 차원의 데이터를 input으로 받아 hiddensize가 d_ff 인 연산 수행\n",
        "  position = PositionalEncoding(d_model, dropout) #  d_model 차원의 positional encoding 수행\n",
        "  model = EncoderDecoder(\n",
        "      Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N), #(self attention + ff) 의 sub layer\n",
        "      Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
        "                            c(ff), dropout), N), #(selfattention +enc_dec attention+ ff) 의 sub layer\n",
        "      nn.Sequential(Embeddings(d_model, src_vocab), c(position)), #source의 embedding vector + position\n",
        "      nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)), # target의 embedding vector + position\n",
        "      Generator(d_model, tgt_vocab)) # output embedding으로부터 softmax 연산 진행하여 최종 예측값 반환\n",
        "  \n",
        "  # This was important from their code. \n",
        "  # Initialize parameters with Glorot / fan_avg.\n",
        "  for p in model.parameters():\n",
        "      if p.dim() > 1:\n",
        "          nn.init.xavier_uniform(p)\n",
        "  return model\n",
        "            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "eIDN1DSd99s-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fc5830f-d54b-49a3-d581-675bbe266bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ]
        }
      ],
      "source": [
        "model = make_model(10,10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljRK80Lo99s_"
      },
      "source": [
        "### Q5. 위 코드로 만든 모델의 모든 파라미터의 이름과 크기 (shape) 을 출력하라"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BHubCUOh99tA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fb64f43-59cc-492f-eacb-539606c9e2c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number 0 Layer name is : encoder.layers.0.self_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 1 Layer name is : encoder.layers.0.self_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 2 Layer name is : encoder.layers.0.self_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 3 Layer name is : encoder.layers.0.self_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 4 Layer name is : encoder.layers.0.self_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 5 Layer name is : encoder.layers.0.self_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 6 Layer name is : encoder.layers.0.self_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 7 Layer name is : encoder.layers.0.self_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 8 Layer name is : encoder.layers.0.feed_forward.w_1.weight,\n",
            " and its shape of parameter vector is torch.Size([2048, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 9 Layer name is : encoder.layers.0.feed_forward.w_1.bias,\n",
            " and its shape of parameter vector is torch.Size([2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 10 Layer name is : encoder.layers.0.feed_forward.w_2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 11 Layer name is : encoder.layers.0.feed_forward.w_2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 12 Layer name is : encoder.layers.0.sublayer.0.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 13 Layer name is : encoder.layers.0.sublayer.0.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 14 Layer name is : encoder.layers.0.sublayer.1.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 15 Layer name is : encoder.layers.0.sublayer.1.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 16 Layer name is : encoder.layers.1.self_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 17 Layer name is : encoder.layers.1.self_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 18 Layer name is : encoder.layers.1.self_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 19 Layer name is : encoder.layers.1.self_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 20 Layer name is : encoder.layers.1.self_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 21 Layer name is : encoder.layers.1.self_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 22 Layer name is : encoder.layers.1.self_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 23 Layer name is : encoder.layers.1.self_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 24 Layer name is : encoder.layers.1.feed_forward.w_1.weight,\n",
            " and its shape of parameter vector is torch.Size([2048, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 25 Layer name is : encoder.layers.1.feed_forward.w_1.bias,\n",
            " and its shape of parameter vector is torch.Size([2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 26 Layer name is : encoder.layers.1.feed_forward.w_2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 27 Layer name is : encoder.layers.1.feed_forward.w_2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 28 Layer name is : encoder.layers.1.sublayer.0.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 29 Layer name is : encoder.layers.1.sublayer.0.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 30 Layer name is : encoder.layers.1.sublayer.1.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 31 Layer name is : encoder.layers.1.sublayer.1.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 32 Layer name is : encoder.layers.2.self_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 33 Layer name is : encoder.layers.2.self_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 34 Layer name is : encoder.layers.2.self_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 35 Layer name is : encoder.layers.2.self_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 36 Layer name is : encoder.layers.2.self_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 37 Layer name is : encoder.layers.2.self_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 38 Layer name is : encoder.layers.2.self_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 39 Layer name is : encoder.layers.2.self_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 40 Layer name is : encoder.layers.2.feed_forward.w_1.weight,\n",
            " and its shape of parameter vector is torch.Size([2048, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 41 Layer name is : encoder.layers.2.feed_forward.w_1.bias,\n",
            " and its shape of parameter vector is torch.Size([2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 42 Layer name is : encoder.layers.2.feed_forward.w_2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 43 Layer name is : encoder.layers.2.feed_forward.w_2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 44 Layer name is : encoder.layers.2.sublayer.0.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 45 Layer name is : encoder.layers.2.sublayer.0.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 46 Layer name is : encoder.layers.2.sublayer.1.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 47 Layer name is : encoder.layers.2.sublayer.1.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 48 Layer name is : encoder.layers.3.self_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 49 Layer name is : encoder.layers.3.self_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 50 Layer name is : encoder.layers.3.self_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 51 Layer name is : encoder.layers.3.self_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 52 Layer name is : encoder.layers.3.self_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 53 Layer name is : encoder.layers.3.self_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 54 Layer name is : encoder.layers.3.self_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 55 Layer name is : encoder.layers.3.self_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 56 Layer name is : encoder.layers.3.feed_forward.w_1.weight,\n",
            " and its shape of parameter vector is torch.Size([2048, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 57 Layer name is : encoder.layers.3.feed_forward.w_1.bias,\n",
            " and its shape of parameter vector is torch.Size([2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 58 Layer name is : encoder.layers.3.feed_forward.w_2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 59 Layer name is : encoder.layers.3.feed_forward.w_2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 60 Layer name is : encoder.layers.3.sublayer.0.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 61 Layer name is : encoder.layers.3.sublayer.0.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 62 Layer name is : encoder.layers.3.sublayer.1.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 63 Layer name is : encoder.layers.3.sublayer.1.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 64 Layer name is : encoder.layers.4.self_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 65 Layer name is : encoder.layers.4.self_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 66 Layer name is : encoder.layers.4.self_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 67 Layer name is : encoder.layers.4.self_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 68 Layer name is : encoder.layers.4.self_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 69 Layer name is : encoder.layers.4.self_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 70 Layer name is : encoder.layers.4.self_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 71 Layer name is : encoder.layers.4.self_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 72 Layer name is : encoder.layers.4.feed_forward.w_1.weight,\n",
            " and its shape of parameter vector is torch.Size([2048, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 73 Layer name is : encoder.layers.4.feed_forward.w_1.bias,\n",
            " and its shape of parameter vector is torch.Size([2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 74 Layer name is : encoder.layers.4.feed_forward.w_2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 75 Layer name is : encoder.layers.4.feed_forward.w_2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 76 Layer name is : encoder.layers.4.sublayer.0.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 77 Layer name is : encoder.layers.4.sublayer.0.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 78 Layer name is : encoder.layers.4.sublayer.1.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 79 Layer name is : encoder.layers.4.sublayer.1.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 80 Layer name is : encoder.layers.5.self_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 81 Layer name is : encoder.layers.5.self_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 82 Layer name is : encoder.layers.5.self_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 83 Layer name is : encoder.layers.5.self_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 84 Layer name is : encoder.layers.5.self_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 85 Layer name is : encoder.layers.5.self_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 86 Layer name is : encoder.layers.5.self_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 87 Layer name is : encoder.layers.5.self_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 88 Layer name is : encoder.layers.5.feed_forward.w_1.weight,\n",
            " and its shape of parameter vector is torch.Size([2048, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 89 Layer name is : encoder.layers.5.feed_forward.w_1.bias,\n",
            " and its shape of parameter vector is torch.Size([2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 90 Layer name is : encoder.layers.5.feed_forward.w_2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 91 Layer name is : encoder.layers.5.feed_forward.w_2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 92 Layer name is : encoder.layers.5.sublayer.0.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 93 Layer name is : encoder.layers.5.sublayer.0.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 94 Layer name is : encoder.layers.5.sublayer.1.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 95 Layer name is : encoder.layers.5.sublayer.1.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 96 Layer name is : encoder.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 97 Layer name is : encoder.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 98 Layer name is : decoder.layers.0.self_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 99 Layer name is : decoder.layers.0.self_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 100 Layer name is : decoder.layers.0.self_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 101 Layer name is : decoder.layers.0.self_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 102 Layer name is : decoder.layers.0.self_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 103 Layer name is : decoder.layers.0.self_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 104 Layer name is : decoder.layers.0.self_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 105 Layer name is : decoder.layers.0.self_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 106 Layer name is : decoder.layers.0.src_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 107 Layer name is : decoder.layers.0.src_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 108 Layer name is : decoder.layers.0.src_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 109 Layer name is : decoder.layers.0.src_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 110 Layer name is : decoder.layers.0.src_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 111 Layer name is : decoder.layers.0.src_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 112 Layer name is : decoder.layers.0.src_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 113 Layer name is : decoder.layers.0.src_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 114 Layer name is : decoder.layers.0.feed_forward.w_1.weight,\n",
            " and its shape of parameter vector is torch.Size([2048, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 115 Layer name is : decoder.layers.0.feed_forward.w_1.bias,\n",
            " and its shape of parameter vector is torch.Size([2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 116 Layer name is : decoder.layers.0.feed_forward.w_2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 117 Layer name is : decoder.layers.0.feed_forward.w_2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 118 Layer name is : decoder.layers.0.sublayer.0.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 119 Layer name is : decoder.layers.0.sublayer.0.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 120 Layer name is : decoder.layers.0.sublayer.1.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 121 Layer name is : decoder.layers.0.sublayer.1.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 122 Layer name is : decoder.layers.0.sublayer.2.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 123 Layer name is : decoder.layers.0.sublayer.2.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 124 Layer name is : decoder.layers.1.self_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 125 Layer name is : decoder.layers.1.self_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 126 Layer name is : decoder.layers.1.self_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 127 Layer name is : decoder.layers.1.self_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 128 Layer name is : decoder.layers.1.self_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 129 Layer name is : decoder.layers.1.self_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 130 Layer name is : decoder.layers.1.self_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 131 Layer name is : decoder.layers.1.self_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 132 Layer name is : decoder.layers.1.src_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 133 Layer name is : decoder.layers.1.src_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 134 Layer name is : decoder.layers.1.src_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 135 Layer name is : decoder.layers.1.src_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 136 Layer name is : decoder.layers.1.src_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 137 Layer name is : decoder.layers.1.src_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 138 Layer name is : decoder.layers.1.src_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 139 Layer name is : decoder.layers.1.src_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 140 Layer name is : decoder.layers.1.feed_forward.w_1.weight,\n",
            " and its shape of parameter vector is torch.Size([2048, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 141 Layer name is : decoder.layers.1.feed_forward.w_1.bias,\n",
            " and its shape of parameter vector is torch.Size([2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 142 Layer name is : decoder.layers.1.feed_forward.w_2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 143 Layer name is : decoder.layers.1.feed_forward.w_2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 144 Layer name is : decoder.layers.1.sublayer.0.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 145 Layer name is : decoder.layers.1.sublayer.0.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 146 Layer name is : decoder.layers.1.sublayer.1.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 147 Layer name is : decoder.layers.1.sublayer.1.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 148 Layer name is : decoder.layers.1.sublayer.2.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 149 Layer name is : decoder.layers.1.sublayer.2.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 150 Layer name is : decoder.layers.2.self_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 151 Layer name is : decoder.layers.2.self_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 152 Layer name is : decoder.layers.2.self_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 153 Layer name is : decoder.layers.2.self_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 154 Layer name is : decoder.layers.2.self_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 155 Layer name is : decoder.layers.2.self_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 156 Layer name is : decoder.layers.2.self_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 157 Layer name is : decoder.layers.2.self_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 158 Layer name is : decoder.layers.2.src_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 159 Layer name is : decoder.layers.2.src_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 160 Layer name is : decoder.layers.2.src_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 161 Layer name is : decoder.layers.2.src_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 162 Layer name is : decoder.layers.2.src_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 163 Layer name is : decoder.layers.2.src_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 164 Layer name is : decoder.layers.2.src_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 165 Layer name is : decoder.layers.2.src_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 166 Layer name is : decoder.layers.2.feed_forward.w_1.weight,\n",
            " and its shape of parameter vector is torch.Size([2048, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 167 Layer name is : decoder.layers.2.feed_forward.w_1.bias,\n",
            " and its shape of parameter vector is torch.Size([2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 168 Layer name is : decoder.layers.2.feed_forward.w_2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 169 Layer name is : decoder.layers.2.feed_forward.w_2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 170 Layer name is : decoder.layers.2.sublayer.0.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 171 Layer name is : decoder.layers.2.sublayer.0.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 172 Layer name is : decoder.layers.2.sublayer.1.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 173 Layer name is : decoder.layers.2.sublayer.1.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 174 Layer name is : decoder.layers.2.sublayer.2.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 175 Layer name is : decoder.layers.2.sublayer.2.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 176 Layer name is : decoder.layers.3.self_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 177 Layer name is : decoder.layers.3.self_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 178 Layer name is : decoder.layers.3.self_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 179 Layer name is : decoder.layers.3.self_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 180 Layer name is : decoder.layers.3.self_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 181 Layer name is : decoder.layers.3.self_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 182 Layer name is : decoder.layers.3.self_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 183 Layer name is : decoder.layers.3.self_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 184 Layer name is : decoder.layers.3.src_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 185 Layer name is : decoder.layers.3.src_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 186 Layer name is : decoder.layers.3.src_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 187 Layer name is : decoder.layers.3.src_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 188 Layer name is : decoder.layers.3.src_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 189 Layer name is : decoder.layers.3.src_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 190 Layer name is : decoder.layers.3.src_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 191 Layer name is : decoder.layers.3.src_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 192 Layer name is : decoder.layers.3.feed_forward.w_1.weight,\n",
            " and its shape of parameter vector is torch.Size([2048, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 193 Layer name is : decoder.layers.3.feed_forward.w_1.bias,\n",
            " and its shape of parameter vector is torch.Size([2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 194 Layer name is : decoder.layers.3.feed_forward.w_2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 195 Layer name is : decoder.layers.3.feed_forward.w_2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 196 Layer name is : decoder.layers.3.sublayer.0.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 197 Layer name is : decoder.layers.3.sublayer.0.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 198 Layer name is : decoder.layers.3.sublayer.1.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 199 Layer name is : decoder.layers.3.sublayer.1.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 200 Layer name is : decoder.layers.3.sublayer.2.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 201 Layer name is : decoder.layers.3.sublayer.2.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 202 Layer name is : decoder.layers.4.self_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 203 Layer name is : decoder.layers.4.self_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 204 Layer name is : decoder.layers.4.self_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 205 Layer name is : decoder.layers.4.self_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 206 Layer name is : decoder.layers.4.self_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 207 Layer name is : decoder.layers.4.self_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 208 Layer name is : decoder.layers.4.self_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 209 Layer name is : decoder.layers.4.self_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 210 Layer name is : decoder.layers.4.src_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 211 Layer name is : decoder.layers.4.src_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 212 Layer name is : decoder.layers.4.src_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 213 Layer name is : decoder.layers.4.src_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 214 Layer name is : decoder.layers.4.src_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 215 Layer name is : decoder.layers.4.src_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 216 Layer name is : decoder.layers.4.src_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 217 Layer name is : decoder.layers.4.src_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 218 Layer name is : decoder.layers.4.feed_forward.w_1.weight,\n",
            " and its shape of parameter vector is torch.Size([2048, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 219 Layer name is : decoder.layers.4.feed_forward.w_1.bias,\n",
            " and its shape of parameter vector is torch.Size([2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 220 Layer name is : decoder.layers.4.feed_forward.w_2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 221 Layer name is : decoder.layers.4.feed_forward.w_2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 222 Layer name is : decoder.layers.4.sublayer.0.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 223 Layer name is : decoder.layers.4.sublayer.0.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 224 Layer name is : decoder.layers.4.sublayer.1.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 225 Layer name is : decoder.layers.4.sublayer.1.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 226 Layer name is : decoder.layers.4.sublayer.2.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 227 Layer name is : decoder.layers.4.sublayer.2.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 228 Layer name is : decoder.layers.5.self_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 229 Layer name is : decoder.layers.5.self_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 230 Layer name is : decoder.layers.5.self_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 231 Layer name is : decoder.layers.5.self_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 232 Layer name is : decoder.layers.5.self_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 233 Layer name is : decoder.layers.5.self_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 234 Layer name is : decoder.layers.5.self_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 235 Layer name is : decoder.layers.5.self_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 236 Layer name is : decoder.layers.5.src_attn.linears.0.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 237 Layer name is : decoder.layers.5.src_attn.linears.0.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 238 Layer name is : decoder.layers.5.src_attn.linears.1.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 239 Layer name is : decoder.layers.5.src_attn.linears.1.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 240 Layer name is : decoder.layers.5.src_attn.linears.2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 241 Layer name is : decoder.layers.5.src_attn.linears.2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 242 Layer name is : decoder.layers.5.src_attn.linears.3.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 243 Layer name is : decoder.layers.5.src_attn.linears.3.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 244 Layer name is : decoder.layers.5.feed_forward.w_1.weight,\n",
            " and its shape of parameter vector is torch.Size([2048, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 245 Layer name is : decoder.layers.5.feed_forward.w_1.bias,\n",
            " and its shape of parameter vector is torch.Size([2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 246 Layer name is : decoder.layers.5.feed_forward.w_2.weight,\n",
            " and its shape of parameter vector is torch.Size([512, 2048])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 247 Layer name is : decoder.layers.5.feed_forward.w_2.bias,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 248 Layer name is : decoder.layers.5.sublayer.0.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 249 Layer name is : decoder.layers.5.sublayer.0.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 250 Layer name is : decoder.layers.5.sublayer.1.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 251 Layer name is : decoder.layers.5.sublayer.1.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 252 Layer name is : decoder.layers.5.sublayer.2.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 253 Layer name is : decoder.layers.5.sublayer.2.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 254 Layer name is : decoder.norm.a_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 255 Layer name is : decoder.norm.b_2,\n",
            " and its shape of parameter vector is torch.Size([512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 256 Layer name is : src_embed.0.lut.weight,\n",
            " and its shape of parameter vector is torch.Size([10, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 257 Layer name is : tgt_embed.0.lut.weight,\n",
            " and its shape of parameter vector is torch.Size([10, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 258 Layer name is : generator.proj.weight,\n",
            " and its shape of parameter vector is torch.Size([10, 512])\n",
            "--------------------------------------------------\n",
            "\n",
            "number 259 Layer name is : generator.proj.bias,\n",
            " and its shape of parameter vector is torch.Size([10])\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 구현\n",
        "\n",
        "for i, (name, param) in enumerate(model.named_parameters()):\n",
        "  print(f'number {i} Layer name is : {name},\\n and its shape of parameter vector is {param.shape}')\n",
        "  print('--------------------------------------------------\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Jesung_Ryu_Week3_4_assginment.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}