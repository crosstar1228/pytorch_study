{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "b7lMU2WDtbQy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "dzS4Kux3tbQ0"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qynSGk1tbQ1"
      },
      "source": [
        "# Pytorch DataLoader\n",
        "Pytorch는 데이터를 전처리하고 배치화할 수 있는 클래스를 제공한다.    \n",
        "`Dataset` 클래스는 데이터를 **전처리**하고 dictionary 또는 list 타입으로 변경할 수 있다.   \n",
        "`DataLoader` 클래스는 데이터 **1. 셔플 2. 배치화 3. 멀티 프로세스** 기능을 제공한다. \n",
        "\n",
        "[OFFICAL DOCUMENT](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
        "\n",
        "# Table of Contents\n",
        "- [Dataset](#Dataset)\n",
        "- [Dataloader](#DataLoader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5LnY3M2tbQ2"
      },
      "source": [
        "## Dataset\n",
        "- 모든 custom dataset 클래스는 `Dataset()` 클래스를 상속받아야 함.\n",
        "- `__getitem__()`와 `__len__()` 메소드를 반드시 오버라이딩해야 함. \n",
        "- `DataLoader` 클래스가 배치를 만들 때 `Dataset` 인스턴스의 `__getitem__()` 메소드를 사용해 데이터에 접근함\n",
        "- 해당 Dataset 클래스는 string sequence 데이터를 **tokenize** & **tensorize**한다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "collapsed": true,
        "id": "EqI5WCTXtbQ2"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from typing import Iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "BQvCa86PtbQ2"
      },
      "outputs": [],
      "source": [
        "trainer_iter = AG_NEWS(split = 'train')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "GBmycbhvnVrK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c65f9e38-934a-430b-e221-56c010f6e076"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__getitem__() function not implemented.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    trainer_iter[0]\n",
        "except NotImplementedError:\n",
        "    print(f\"__getitem__() function not implemented.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "2BHquGVw3RTD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "681117c8-e9b2-42b3-ebc5-2503432f3e9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,\n",
              " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "next(trainer_iter)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Dataset Class를 상속받아 Custom_Dataset을 반환하는 과정"
      ],
      "metadata": {
        "id": "byOEj7VihjNt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "sZ0s2hgatbQ3"
      },
      "outputs": [],
      "source": [
        "class Custom_Dataset(Dataset):\n",
        "\n",
        "    def __init__(self, data: Iterator):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "        self.target = []\n",
        "        self.text = []\n",
        "        for target, text in data:\n",
        "            self.target.append(target)\n",
        "            self.text.append(text)\n",
        "  \n",
        "    def __len__(self):\n",
        "        return len(self.target)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # encode\n",
        "        token_ids = self.tokenizer.encode(\n",
        "        text = self.text[index],\n",
        "        truncation = True,\n",
        "        padding='max_length'\n",
        "        )\n",
        "        \n",
        "        # tensorize\n",
        "        return torch.tensor(token_ids), torch.tensor([self.target[index]])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "mIwzIDNKl8MZ"
      },
      "outputs": [],
      "source": [
        "train_dataset = Custom_Dataset(trainer_iter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "hG5KwzljmGxZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc7eba48-80fa-469f-9705-aaed724fe846"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([  101, 19879,  1513, 15218, 27674, 10472, 19417,   113, 11336, 27603,\n",
              "           114, 11336, 27603,   118,  7219,  5151,  3016, 19879,  1513,  1990,\n",
              "           117,   165,  1134,  1144,   170,  5244,  1111,  1543,  1218,   118,\n",
              "         25457,  1105,  5411,   165,  6241,  2399,  1107,  1103,  3948,  2380,\n",
              "           117,  1144,  4432,  1973,   165,  1157,  7023,  1116,  1113,  1330,\n",
              "          1226,  1104,  1103,  2319,   119,   102,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0]), tensor([3]))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "VPn5C8B9n7Pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bb6fd24-d712-4c43-b1a3-55700f9d4b46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "119999"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "GIl5vcsvm5oB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "67659060-f85d-4966-86c2-2b3e20a08161"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'[CLS] Carlyle Looks Toward Commercial Aerospace ( Reuters ) Reuters - Private investment firm Carlyle Group, \\\\ which has a reputation for making well - timed and occasionally \\\\ controversial plays in the defense industry, has quietly placed \\\\ its bets on another part of the market. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "# decode to see original text\n",
        "train_dataset.tokenizer.decode(train_dataset[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "metadata": {
        "id": "HaCzvFOVwC6Y"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer_bert = BertTokenizer.from_pretrained(\"klue/bert-base\")"
      ],
      "metadata": {
        "id": "xKgIH8a2wBcJ"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhgHYusAtbQ3"
      },
      "source": [
        "## Dataloader\n",
        "\n",
        "-[DataLoader pytorch official document](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)   \n",
        "-[collate_fn 설명 영문 블로그](https://androidkt.com/create-dataloader-with-collate_fn-for-variable-length-input-in-pytorch/)  \n",
        "\n",
        "- `dataset`\n",
        "    - **map-style** dataset\n",
        "    (`Dataset`)\n",
        "    - iterable style dataset\n",
        "      - `__iter__()`\n",
        "- `batch_size` \n",
        "  - int\n",
        "- `shuffle`\n",
        "  - bool\n",
        "- `sampler`\n",
        "  - data index 이터레이터\n",
        "  - `Dataset` 및 `DataLoader` 모듈과 같이 별도의 import 필요\n",
        "  - `RandomSampler(dataset)`: batch 별 random하게 sampling\n",
        "  - `SequentialSampler(dataset)`: batch 별 sequential하게 sampling\n",
        "\n",
        "- `collate_fn`\n",
        "  - List(tuple)의 형태를 입력으로 받는 전처리 함수\n",
        "  - dataloader에 keyword argument로 함수명을 입력함으로서 적용 가능\n",
        "  - customizing해서 정의하는 것이 일반적\n",
        "  - nlp의 경우 (text, label) 형태의 tuple을 입력으로 받아 tokenizing 및 tensorizing 과정을 수행하여 output으로 반환\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def custom_collate_fn(batch):\n",
        "#   \"\"\"\n",
        "#   - batch: list of tuples (input_data(string), target_data(int))\n",
        "  \n",
        "#   한 배치 내 문장들을 tokenizing 한 후 텐서로 변환함. \n",
        "#   이때, dynamic padding (즉, 같은 배치 내 토큰의 개수가 동일할 수 있도록, 부족한 문장에 [PAD] 토큰을 추가하는 작업)을 적용\n",
        "#   토큰 개수는 배치 내 가장 긴 문장으로 해야함.\n",
        "#   또한 최대 길이를 넘는 문장은 최대 길이 이후의 토큰을 제거하도록 해야 함\n",
        "#   토크나이즈된 결과 값은 텐서 형태로 반환하도록 해야 함\n",
        "  \n",
        "#   한 배치 내 레이블(target)은 텐서화 함.\n",
        "  \n",
        "#   (input, target) 튜플 형태를 반환.\n",
        "#   \"\"\"\n",
        "#   global tokenizer_bert\n",
        "  \n",
        "#   input_list, target_list = zip(*batch) #tuple 별 [0] ,[1] 원소를 unpacking 후 zip으로 할당\n",
        "  \n",
        "#   tensorized_input = tokenizer_bert(\n",
        "#       text=input_list,\n",
        "#       padding='longest',\n",
        "#       add_special_tokens=True,\n",
        "#       truncation=True, #longest는 최대길이를 초과함\n",
        "#       return_tensors='pt'\n",
        "#   )\n",
        "  \n",
        "#   tensorized_label = torch.tensor(target_list)\n",
        "  \n",
        "#   return tensorized_input, tensorized_label\n",
        "    "
      ],
      "metadata": {
        "id": "SxhpI8h2viQo"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "nOh5gczRtbQ4"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import RandomSampler, SequentialSampler\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size = 32, sampler= RandomSampler(train_dataset))#, collate_fn = custom_collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dataloader로부터 batch size 확인\n",
        "cnt=0\n",
        "for x, y in train_dataloader:\n",
        "  print(x.shape, y.shape)\n",
        "  cnt+=1\n",
        "  if cnt==10:\n",
        "    break;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HilF4PR0wDiV",
        "outputId": "c757fb9a-4517-4cc0-e5a0-a0acfb346900"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 512]) torch.Size([32, 1])\n",
            "torch.Size([32, 512]) torch.Size([32, 1])\n",
            "torch.Size([32, 512]) torch.Size([32, 1])\n",
            "torch.Size([32, 512]) torch.Size([32, 1])\n",
            "torch.Size([32, 512]) torch.Size([32, 1])\n",
            "torch.Size([32, 512]) torch.Size([32, 1])\n",
            "torch.Size([32, 512]) torch.Size([32, 1])\n",
            "torch.Size([32, 512]) torch.Size([32, 1])\n",
            "torch.Size([32, 512]) torch.Size([32, 1])\n",
            "torch.Size([32, 512]) torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "0KcSb62xxCpZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "[week2-3]pytorch_dataloader.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "torch"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}